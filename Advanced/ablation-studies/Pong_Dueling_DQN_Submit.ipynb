{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "gsUpoEHdZ-hx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU_5-BKk8tzG"
      },
      "outputs": [],
      "source": [
        "!pip install gym\\[atari,accept-rom-license\\]==0.21.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "D_Ijs1QeaFUa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0M_2qhJ7Skl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import resize\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import os, dill\n",
        "import random, time\n",
        "import gym\n",
        "\n",
        "from collections import namedtuple, deque\n",
        "from copy import deepcopy, copy\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv8GkXFgSP8y",
        "outputId": "b398e011-c19b-4b24-85ca-a8ce3e35bd73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbwemBiS72PA",
        "outputId": "b121a4a0-fa1b-49e3-f5b5-cf1dfea83fbb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrapper class for Pong\n",
        "\n",
        "[source](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/wrappers.py)"
      ],
      "metadata": {
        "id": "VVzjG0yIaMQF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N67PnOzkkImh"
      },
      "outputs": [],
      "source": [
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        return obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh3uCEZt78K4"
      },
      "source": [
        "# Explore environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "T3pIm8NV780w",
        "outputId": "652595b1-2258-4102-a8bf-904e1d419a83"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = gym.make(\"PongDeterministic-v4\")\n",
        "env = FireResetEnv(env)\n",
        "state = env.reset()\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.imshow(state)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdGp1Tlw9pe3",
        "outputId": "d0846ae9-5af8-4c2b-96c2-ca9af88569f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of possible actions: 6\n"
          ]
        }
      ],
      "source": [
        "print(f'Number of possible actions: {env.action_space.n}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMn3waeav1IK"
      },
      "outputs": [],
      "source": [
        "def play_one_episode():\n",
        "    done = False\n",
        "    s = env.reset()\n",
        "    reward = 0\n",
        "    while not done:\n",
        "        s, r, done, _ = env.step(env.action_space.sample())\n",
        "        reward += r\n",
        "\n",
        "    return reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOjezIsU-BhN"
      },
      "source": [
        "# Preprocessing:\n",
        "\n",
        "1. Convert to grayscale.\n",
        "2. Ressize to 75 x 80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE72hyAe-JlZ"
      },
      "outputs": [],
      "source": [
        "# # convert to gray scale\n",
        "# def convert_to_gray(img):\n",
        "#     return np.dot(img, [0.2989, 0.5870, 0.1140])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[source](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)"
      ],
      "metadata": {
        "id": "zwyq26RmbD0R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NkfOJ__kOU2"
      },
      "outputs": [],
      "source": [
        "def convert_to_gray_rescale(img):\n",
        "    \"\"\" prepro 210x160x3 uint8 frame into 6000 (75x80) 1D float vector \"\"\"\n",
        "    img = img[35:185] # crop - remove 35px from start & 25px from end of image in x, to reduce redundant parts of image (i.e. after ball passes paddle)\n",
        "    img = img[::2,::2,0] # downsample by factor of 2.\n",
        "    img[img == 144] = 0 # erase background (background type 1)\n",
        "    img[img == 109] = 0 # erase background (background type 2)\n",
        "    img[img != 0] = 1 # everything else (paddles, ball) just set to 1. this makes the image grayscale effectively\n",
        "    return img # ravel flattens an array and collapses it into a column vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "t5YIepNL_GbB",
        "outputId": "d41ebf67-42aa-4527-9d33-04d5ed4a7d26"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "state_g = convert_to_gray_rescale(state)\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.imshow(state_g, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGKxFBjHx3zR"
      },
      "outputs": [],
      "source": [
        "def normalize(img):\n",
        "    return img / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrReW_0MAHr_"
      },
      "outputs": [],
      "source": [
        "# putting everything together\n",
        "def preprocess(img):\n",
        "    img_g = convert_to_gray_rescale(img)\n",
        "    #img_t = resize(img_g, (84, 84))\n",
        "    img_n = normalize(img_g)\n",
        "    return img_n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replay Buffer Class"
      ],
      "metadata": {
        "id": "dkNpgajpbPOM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU6rtZXwAq4J"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, env, capacity, n_frames):\n",
        "        self.env = env\n",
        "        self.capacity = capacity\n",
        "        self.n_frames = n_frames\n",
        "        self.buffer = namedtuple('Buffer', field_names=['state', 'action', 'reward', 'done','next_state'])\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "        \n",
        "    def push(self, state, action, reward, done, next_state):\n",
        "        # convert (state, action, next_state, reward, done)\n",
        "        self.memory.append(self.buffer(state, action, reward, done, next_state))\n",
        "        \n",
        "        \n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.memory), batch_size, replace=False)\n",
        "        batch = zip(*[self.memory[i] for i in indices])\n",
        "        return batch\n",
        "    \n",
        "    def populate(self, length):\n",
        "        # populate buffer\n",
        "        state_frames = deque(maxlen=self.n_frames)\n",
        "        next_state_frames = deque(maxlen=self.n_frames)\n",
        "        zeros = np.zeros((75,80))\n",
        "\n",
        "\n",
        "        while len(self.memory) < length:\n",
        "            # init frames\n",
        "            for i in range(self.n_frames):\n",
        "                state_frames.append(zeros)\n",
        "                next_state_frames.append(zeros)\n",
        "            \n",
        "            done = False\n",
        "            s0 = self.env.reset()\n",
        "            while not done:\n",
        "                action = self.env.action_space.sample()\n",
        "                s1, r, done, _ = self.env.step(action)\n",
        "\n",
        "                # build input states\n",
        "                s0_t = preprocess(s0) # (84, 84)\n",
        "                state_frames.append(s0_t) # stack of 4 (84,84)\n",
        "                s1_t = preprocess(s1) # (84, 84)\n",
        "                next_state_frames.append(s1_t) # stack of 4 (84,84)\n",
        "\n",
        "                # stack 4 frames in  tensor\n",
        "                input_state = np.stack(state_frames)\n",
        "                input_next_state = np.stack(next_state_frames)\n",
        "\n",
        "                # update buffer\n",
        "                self.push(input_state, action, r, done, input_next_state)\n",
        "                s0 = s1\n",
        "\n",
        "                if len(self.memory) >= length:\n",
        "                    break\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dueling Q-Network Class"
      ],
      "metadata": {
        "id": "LmgE4V7BbXfE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVqgc3tNQF4Z"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self,env, n_frames):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(n_frames, 32, kernel_size=8, stride=4), # input_shape: batch_size x 4 x 84 x 84 -> batch_size x 32 x 20 x 20\n",
        "            #nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            #nn.BatchNorm2d(64),\n",
        "            nn.ReLU(), \n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            #nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            )\n",
        "        \n",
        "        conv_out_shape = self._get_input_shape([4,75,80]) # \n",
        "\n",
        "        self.value_stream = nn.Sequential(\n",
        "            nn.Linear(conv_out_shape, 512),\n",
        "            #nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "        self.advantage_stream = nn.Sequential(\n",
        "            nn.Linear(conv_out_shape, 512),\n",
        "            #nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, env.action_space.n)\n",
        "        )\n",
        "              \n",
        "    def forward(self, x):\n",
        "        # conv layer output\n",
        "        features = self.conv(x)\n",
        "        features = features.view(x.size(0), -1)\n",
        "\n",
        "        # value stream\n",
        "        values = self.value_stream(features) \n",
        "\n",
        "        # advantages\n",
        "        advantages = self.advantage_stream(features)\n",
        "\n",
        "        # q values\n",
        "        q_values = values + (advantages - advantages.mean())\n",
        "\n",
        "        return q_values\n",
        "        \n",
        "    def _get_input_shape(self, shape):\n",
        "        conv_out = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(conv_out.size()))\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent Class"
      ],
      "metadata": {
        "id": "FXKXAcxebaby"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73c5BoOMTR_E"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self, env, buffer, epsilon, epsilon_mid, epsilon_final, decay_mid, decay_final):\n",
        "        self.env = env\n",
        "        self.buffer = buffer\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_start = epsilon\n",
        "        self.epsilon_mid = epsilon_mid\n",
        "        self.epsilon_final = epsilon_final\n",
        "        self.decay_mid = decay_mid\n",
        "        self.decay_final = decay_final\n",
        "        self.state_frames = deque(maxlen=buffer.n_frames)\n",
        "        self.next_state_frames = deque(maxlen=buffer.n_frames)\n",
        "        self.reset_frames()\n",
        "        \n",
        "    def take_action(self, state):\n",
        "        \n",
        "        # build state frames\n",
        "        state_frames = self.build_state_frames(state) # 4, 75, 80\n",
        "        # which action to take ?\n",
        "        action = self.get_action(state_frames)\n",
        "\n",
        "        # take the action\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "        # build next state frames\n",
        "        next_state_frames = self.build_next_state_frames(next_state)\n",
        "\n",
        "        # push new buffer\n",
        "        self.buffer.push(state_frames, action, reward, done, next_state_frames)\n",
        "        \n",
        "        return next_state, reward, done\n",
        "        \n",
        "    def get_action(self, state_frames):\n",
        "        if random.random() < self.epsilon:\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            action = self.get_greedy_action(state_frames)\n",
        "        return action\n",
        "            \n",
        "    def get_greedy_action(self, state_frames):\n",
        "        with torch.no_grad():\n",
        "            Q_network.eval()\n",
        "            state_frames = np.expand_dims(state_frames, 0)\n",
        "            state_frames = torch.FloatTensor(state_frames).to(device)\n",
        "            index_action = torch.argmax(Q_network(state_frames).cpu()).item()\n",
        "            Q_network.train()\n",
        "        return index_action\n",
        "    \n",
        "    \n",
        "    def learn(self, batch_size):\n",
        "        # sample a batch\n",
        "        batch = self.buffer.sample(batch_size)\n",
        "        states, actions, rewards, dones, next_states = [i for i in batch]\n",
        "\n",
        "        # transform batch to tensor then to device\n",
        "        states_t = torch.FloatTensor(np.array(states)).to(device)\n",
        "        actions_t = torch.LongTensor(np.array(actions)).to(device)\n",
        "        rewards_t = torch.FloatTensor(np.array(rewards)).to(device)\n",
        "        dones_t = torch.BoolTensor(np.array(dones)).to(device)\n",
        "        next_states_t = torch.FloatTensor(np.array(next_states)).to(device)\n",
        "\n",
        "        # update rule : q(s,a) = q(s,a) + (r + gamma * max_a'_q(s',a') - q(s,a))\n",
        "        # y = r + gamma * max_a'_q(s',a')\n",
        "        # get q_vals target\n",
        "        next_q_values = Q_network(next_states_t).detach().max(1)[0]# batch size, 1\n",
        "        next_q_values[dones_t] = 0 # zero the final states\n",
        "        y = rewards_t + gamma * next_q_values\n",
        "        y = y.unsqueeze(1)\n",
        "    \n",
        "        # q_values\n",
        "        q_values = Q_network(states_t).gather(1, actions_t.reshape(-1, 1))\n",
        "        \n",
        "        # compute loss\n",
        "        loss = criterion(q_values, y)\n",
        "        \n",
        "        # back prop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # # Trick: gradient clipping\n",
        "        for param in Q_network.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "\n",
        "        # gradient descent\n",
        "        optimizer.step()\n",
        "        \n",
        "        return loss.item()\n",
        "         \n",
        "    def reset_frames(self):\n",
        "        zeros = np.zeros((75,80))\n",
        "        for i in range(self.buffer.n_frames):\n",
        "            self.state_frames.append(zeros)\n",
        "            self.next_state_frames.append(zeros)\n",
        "\n",
        "    def build_state_frames(self, state):\n",
        "        state_t = preprocess(state)\n",
        "        self.state_frames.append(state_t)\n",
        "        return np.stack(self.state_frames)\n",
        "\n",
        "    def build_next_state_frames(self, next_state):\n",
        "        next_state_t = preprocess(next_state)\n",
        "        self.next_state_frames.append(next_state_t)\n",
        "        return np.stack(self.next_state_frames)\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        e = self.epsilon \n",
        "\n",
        "        if e > self.epsilon_mid:\n",
        "            self.epsilon = max(self.epsilon_mid, e - self.decay_mid)\n",
        "        else:\n",
        "            self.epsilon = max(self.epsilon_final, e - self.decay_final)\n",
        " \n",
        "    def reset_epsilon(self):\n",
        "        self.epsilon = self.epsilon_start"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standard Weight Initialization function"
      ],
      "metadata": {
        "id": "iA3mkHaBbgyI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBEf6oAZiv8p"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(m):\n",
        "  if isinstance(m, nn.Conv2d):\n",
        "      nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
        "      if m.bias is not None:\n",
        "          nn.init.constant_(m.bias.data, 0)\n",
        "  elif isinstance(m, nn.BatchNorm2d):\n",
        "      nn.init.constant_(m.weight.data, 1)\n",
        "      nn.init.constant_(m.bias.data, 0)\n",
        "  elif isinstance(m, nn.Linear):\n",
        "      nn.init.kaiming_uniform_(m.weight.data)\n",
        "      nn.init.constant_(m.bias.data, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "5-attCTabmtW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z79ssfZ1kFcE"
      },
      "outputs": [],
      "source": [
        "# Hyper params \n",
        "capacity = 40000\n",
        "n_frames = 4\n",
        "epsilon = 1\n",
        "epsilon_mid = 0.1\n",
        "epsilon_final = 0.02\n",
        "initial_buffer_size = 10000\n",
        "mid_exploration_frame = 1000000\n",
        "final_exploration_frame = 2400000\n",
        "\n",
        "decay_mid =  (epsilon - epsilon_mid) / mid_exploration_frame\n",
        "\n",
        "decay_final = (epsilon_mid - epsilon_final) / (final_exploration_frame - mid_exploration_frame)\n",
        "\n",
        "\n",
        "lr = 1e-4 #25e-5 # ok for pong\n",
        "batch_size = 32\n",
        "\n",
        "max_episodes = 5000\n",
        "rewards_target = 19\n",
        "window = 100\n",
        "network_update_frequency = 4\n",
        "network_sync_frequency = 1000\n",
        "network_epsilon_update_frequency = 1\n",
        "gamma = 0.99\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instatiate and Populate Replay Buffer"
      ],
      "metadata": {
        "id": "UrTn8Xh9bpcS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW9vFtEykR6Q",
        "outputId": "feaf115a-dc62-478f-868a-32b680f4d28a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It took : 0:00:08.228255 to populate buffer of size: 10000\n"
          ]
        }
      ],
      "source": [
        "buffer = ReplayBuffer(env, capacity, n_frames)\n",
        "t0 = datetime.now()\n",
        "buffer.populate(initial_buffer_size)\n",
        "dt = datetime.now() - t0\n",
        "\n",
        "print(\"It took :\", dt, \"to populate buffer of size:\", len(buffer))\n",
        "agent = Agent(env, buffer, epsilon, epsilon_mid, epsilon_final, decay_mid, decay_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enfzqHLor0jN"
      },
      "source": [
        "# Save / Load functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BQHFrTTrz1Z"
      },
      "outputs": [],
      "source": [
        "drive_path = \"/content/drive/MyDrive/PongDeterministic-v4-Dueling-Only\"\n",
        "\n",
        "if not os.path.exists(drive_path):\n",
        "    os.mkdir(drive_path)\n",
        "data_names = ['models', 'mean_rewards', 'best_score', 'losses']\n",
        "def save_data(episode, reward):\n",
        "\n",
        "    print(\"saving to drive\")\n",
        "\n",
        "    for data in data_names:\n",
        "        data_path = os.path.join(drive_path, data)\n",
        "        if not os.path.exists(data_path):\n",
        "            os.mkdir(data_path)\n",
        "\n",
        "        if data == 'models':\n",
        "\n",
        "            data_file_name = './pong_ep_{}_score_{}.pth'.format(episode, reward)\n",
        "            data_file_path = os.path.join(data_path, data_file_name)\n",
        "\n",
        "            torch.save(Q_network, data_file_path)\n",
        "            print('model saved')        \n",
        "\n",
        "        else:\n",
        "            print(\"Saving \" + data )\n",
        "            data_file_name = data + f\"_{episode}_{reward}.pkl\"\n",
        "            data_file_path = os.path.join(data_path, data_file_name)\n",
        "\n",
        "            with open(data_file_path, 'wb') as f:\n",
        "                if data == 'mean_rewards':\n",
        "                    dill.dump(mean_episode_rewards, f)\n",
        "                elif data == 'best_score':\n",
        "                    dill.dump(best_score, f)\n",
        "                elif data == 'losses':\n",
        "                    dill.dump(episodes_loss, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wle85688r6TZ"
      },
      "outputs": [],
      "source": [
        "def load_data(episode, reward):\n",
        "\n",
        "    print(\"loading from drive\")\n",
        "\n",
        "    for data in data_names:\n",
        "        data_path = os.path.join(drive_path, data)\n",
        "\n",
        "        if data == 'models':\n",
        "            data_file_name = f\"pong_ep_{episode}_score_{reward}.pth\"\n",
        "            data_file_path = os.path.join(data_path, data_file_name)\n",
        "\n",
        "            Q_network = torch.load(data_file_path, map_location=device)\n",
        "            Q_target = torch.load(data_file_path, map_location=device)\n",
        "            print(\"Models Loaded\")\n",
        "            print(Q_network)\n",
        "        \n",
        "        else:\n",
        "            print(\"Loading \" + data)\n",
        "            data_file_name = data + f\"_{episode}_{reward}.pkl\"\n",
        "            data_file_path = os.path.join(data_path, data_file_name)\n",
        "            with open(data_file_path, 'rb') as f:\n",
        "                if data == 'mean_rewards':\n",
        "                    mean_episode_rewards = dill.load(f)\n",
        "                    print(\"Rewards loaded, length of rewards:\", len(mean_episode_rewards))\n",
        "                elif data == 'best_score':\n",
        "                    best_score = dill.load(f)\n",
        "                    print(\"best score loaded:\", best_score)\n",
        "                elif data == 'losses':\n",
        "                    episodes_loss = dill.load(f)\n",
        "                    print(\"episodes_loss loaded, length of losses:\", len(episodes_loss))\n",
        "\n",
        "    return Q_network, Q_target, mean_episode_rewards, best_score, episodes_loss\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate Network"
      ],
      "metadata": {
        "id": "7E7yazrub8_U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnvkRiYKZE0L",
        "outputId": "a2082c70-ab14-474a-89a4-eae77404a759"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QNetwork(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
              "    (3): ReLU()\n",
              "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (5): ReLU()\n",
              "  )\n",
              "  (value_stream): Sequential(\n",
              "    (0): Linear(in_features=1920, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
              "  )\n",
              "  (advantage_stream): Sequential(\n",
              "    (0): Linear(in_features=1920, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Q_network = QNetwork(env, n_frames)\n",
        "Q_network.apply(initialize_weights)\n",
        "Q_network.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3cU5lRO2u3q"
      },
      "outputs": [],
      "source": [
        "# In case loading a pretrained model\n",
        "# Q_network, _ , mean_episode_rewards, best_score, episodes_loss = load_data(6237,1741)\n",
        "# Q_network.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "5LHiA1IVcMN_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDhuZX17eCWA"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(Q_network.parameters(), lr=lr)\n",
        "#optimizer = torch.optim.RMSprop(Q_network.parameters(), lr=lr)\n",
        "criterion = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujsong_IZGP1",
        "outputId": "176d6426-6d2f-448d-abec-903a26b7a578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0 Mean Rewards -21.00 \n",
            "Total steps so far: 884\n",
            "Current epsilon: 1\n",
            "Episode 10 Mean Rewards -20.64 \n",
            "Episode 20 Mean Rewards -20.38 \n",
            "Episode 30 Mean Rewards -20.32 \n",
            "Episode 40 Mean Rewards -20.17 \n",
            "Episode 50 Mean Rewards -20.27 \n",
            "Episode 60 Mean Rewards -20.28 \n",
            "Episode 70 Mean Rewards -20.34 \n",
            "Episode 80 Mean Rewards -20.35 \n",
            "Episode 90 Mean Rewards -20.37 \n",
            "Episode 100 Mean Rewards -20.39 \n",
            "Total steps so far: 93429\n",
            "Current epsilon: 0.9609130000012869\n",
            "Episode 110 Mean Rewards -20.39 \n",
            "Episode 120 Mean Rewards -20.44 \n",
            "Episode 130 Mean Rewards -20.46 \n",
            "Episode 140 Mean Rewards -20.51 \n",
            "Episode 150 Mean Rewards -20.50 \n",
            "Episode 160 Mean Rewards -20.49 \n",
            "Episode 170 Mean Rewards -20.46 \n",
            "Episode 180 Mean Rewards -20.47 \n",
            "Episode 190 Mean Rewards -20.48 \n",
            "Episode 200 Mean Rewards -20.43 \n",
            "Total steps so far: 185629\n",
            "Current epsilon: 0.8779330000040189\n",
            "Episode 210 Mean Rewards -20.41 \n",
            "Episode 220 Mean Rewards -20.39 \n",
            "Episode 230 Mean Rewards -20.42 \n",
            "Episode 240 Mean Rewards -20.40 \n",
            "Episode 250 Mean Rewards -20.37 \n",
            "Episode 260 Mean Rewards -20.38 \n",
            "Episode 270 Mean Rewards -20.37 \n",
            "Episode 280 Mean Rewards -20.35 \n",
            "Episode 290 Mean Rewards -20.27 \n",
            "Episode 300 Mean Rewards -20.30 \n",
            "Total steps so far: 279748\n",
            "Current epsilon: 0.7932259000068077\n",
            "Episode 310 Mean Rewards -20.31 \n",
            "Episode 320 Mean Rewards -20.32 \n",
            "Episode 330 Mean Rewards -20.26 \n",
            "Episode 340 Mean Rewards -20.29 \n",
            "Episode 350 Mean Rewards -20.28 \n",
            "Episode 360 Mean Rewards -20.19 \n",
            "Episode 370 Mean Rewards -20.17 \n",
            "Episode 380 Mean Rewards -20.10 \n",
            "Episode 390 Mean Rewards -20.09 \n",
            "Episode 400 Mean Rewards -20.04 \n",
            "Total steps so far: 376681\n",
            "Current epsilon: 0.7059862000096799\n",
            "Episode 410 Mean Rewards -20.02 \n",
            "Episode 420 Mean Rewards -19.95 \n",
            "Episode 430 Mean Rewards -19.90 \n",
            "Episode 440 Mean Rewards -19.86 \n",
            "Episode 450 Mean Rewards -19.85 \n",
            "Episode 460 Mean Rewards -19.84 \n",
            "Episode 470 Mean Rewards -19.79 \n",
            "Episode 480 Mean Rewards -19.81 \n",
            "Episode 490 Mean Rewards -19.77 \n",
            "Episode 500 Mean Rewards -19.63 \n",
            "Total steps so far: 481208\n",
            "Current epsilon: 0.6119119000127772\n",
            "Episode 510 Mean Rewards -19.52 \n",
            "Episode 520 Mean Rewards -19.40 \n",
            "Episode 530 Mean Rewards -19.29 \n",
            "Episode 540 Mean Rewards -19.26 \n",
            "Episode 550 Mean Rewards -19.16 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 556398. New best score: -18.99\n",
            "Episode 560 Mean Rewards -18.99 \n",
            "Episode 570 Mean Rewards -18.97 \n",
            "Episode 580 Mean Rewards -18.87 \n",
            "Episode 590 Mean Rewards -18.88 \n",
            "Episode 600 Mean Rewards -18.87 \n",
            "Total steps so far: 611735\n",
            "Current epsilon: 0.4944376000163017\n",
            "Episode 610 Mean Rewards -18.74 \n",
            "Episode 620 Mean Rewards -18.77 \n",
            "Episode 630 Mean Rewards -18.76 \n",
            "Episode 640 Mean Rewards -18.67 \n",
            "Episode 650 Mean Rewards -18.48 \n",
            "Episode 660 Mean Rewards -18.55 \n",
            "Episode 670 Mean Rewards -18.39 \n",
            "Episode 680 Mean Rewards -18.25 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 736959. New best score: -18.17\n",
            "Episode 690 Mean Rewards -18.10 \n",
            "Episode 700 Mean Rewards -17.94 \n",
            "Total steps so far: 767487\n",
            "Current epsilon: 0.35426080001227084\n",
            "Episode 710 Mean Rewards -17.85 \n",
            "Episode 720 Mean Rewards -17.64 \n",
            "Episode 730 Mean Rewards -17.59 \n",
            "Episode 740 Mean Rewards -17.47 \n",
            "Episode 750 Mean Rewards -17.55 \n",
            "Episode 760 Mean Rewards -17.60 \n",
            "Episode 770 Mean Rewards -17.39 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 899564. New best score: -17.36\n",
            "Episode 780 Mean Rewards -17.28 \n",
            "Episode 790 Mean Rewards -17.05 \n",
            "Episode 800 Mean Rewards -16.89 \n",
            "Total steps so far: 955547\n",
            "Current epsilon: 0.1850068000094082\n",
            "Episode 810 Mean Rewards -16.75 \n",
            "Episode 820 Mean Rewards -16.76 \n",
            "Episode 830 Mean Rewards -16.75 \n",
            "Episode 840 Mean Rewards -17.01 \n",
            "Episode 850 Mean Rewards -16.62 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1052450. New best score: -16.52\n",
            "Episode 860 Mean Rewards -16.19 \n",
            "Episode 870 Mean Rewards -15.99 \n",
            "Episode 880 Mean Rewards -15.90 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1134498. New best score: -15.69\n",
            "Episode 890 Mean Rewards -15.67 \n",
            "Episode 900 Mean Rewards -15.10 \n",
            "Total steps so far: 1186702\n",
            "Current epsilon: 0.09218845714317456\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1200627. New best score: -14.83\n",
            "Episode 910 Mean Rewards -14.69 \n",
            "Episode 920 Mean Rewards -14.16 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1257828. New best score: -13.78\n",
            "Episode 930 Mean Rewards -13.08 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1292189. New best score: -12.86\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1318084. New best score: -12.04\n",
            "Episode 940 Mean Rewards -11.85 \n",
            "Episode 950 Mean Rewards -11.44 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1374029. New best score: -11.2\n",
            "Episode 960 Mean Rewards -10.81 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1423008. New best score: -10.36\n",
            "Episode 970 Mean Rewards -10.25 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1472380. New best score: -9.43\n",
            "Episode 980 Mean Rewards -9.43 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1512041. New best score: -8.55\n",
            "Episode 990 Mean Rewards -8.55 \n",
            "Episode 1000 Mean Rewards -7.96 \n",
            "Total steps so far: 1552357\n",
            "Current epsilon: 0.07129388571545214\n",
            "Check point\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1565079. New best score: -7.7\n",
            "Episode 1010 Mean Rewards -7.05 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1606864. New best score: -6.81\n",
            "Episode 1020 Mean Rewards -6.34 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1654478. New best score: -5.92\n",
            "Episode 1030 Mean Rewards -5.67 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1705725. New best score: -5.04\n",
            "Episode 1040 Mean Rewards -5.12 \n",
            "Episode 1050 Mean Rewards -4.59 \n",
            "Episode 1060 Mean Rewards -4.32 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1800419. New best score: -4.2\n",
            "Episode 1070 Mean Rewards -4.06 \n",
            "Episode 1080 Mean Rewards -3.72 \n",
            "Episode 1090 Mean Rewards -3.50 \n",
            "Episode 1100 Mean Rewards -3.44 \n",
            "Total steps so far: 1953234\n",
            "Current epsilon: 0.0483866285735258\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1980701. New best score: -3.32\n",
            "Episode 1110 Mean Rewards -3.45 \n",
            "Episode 1120 Mean Rewards -3.82 \n",
            "Episode 1130 Mean Rewards -4.42 \n",
            "Episode 1140 Mean Rewards -4.74 \n",
            "Episode 1150 Mean Rewards -4.63 \n",
            "Episode 1160 Mean Rewards -3.95 \n",
            "Episode 1170 Mean Rewards -3.90 \n",
            "Episode 1180 Mean Rewards -4.35 \n",
            "Episode 1190 Mean Rewards -4.27 \n",
            "Episode 1200 Mean Rewards -4.01 \n",
            "Total steps so far: 2385960\n",
            "Current epsilon: 0.02365942857406968\n",
            "Episode 1210 Mean Rewards -3.26 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2472577. New best score: -2.4\n",
            "Episode 1220 Mean Rewards -2.22 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2513273. New best score: -1.58\n",
            "Episode 1230 Mean Rewards -1.37 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2558461. New best score: -0.77\n",
            "Episode 1240 Mean Rewards -0.59 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2606192. New best score: 0.09\n",
            "Episode 1250 Mean Rewards 0.18 \n",
            "Episode 1260 Mean Rewards 0.37 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2710841. New best score: 0.92\n",
            "Episode 1270 Mean Rewards 1.01 \n",
            "Episode 1280 Mean Rewards 1.61 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2800494. New best score: 1.75\n",
            "Episode 1290 Mean Rewards 1.63 \n",
            "Episode 1300 Mean Rewards 1.84 \n",
            "Total steps so far: 2876371\n",
            "Episode 1310 Mean Rewards 1.54 \n",
            "Episode 1320 Mean Rewards 1.59 \n",
            "Episode 1330 Mean Rewards 2.04 \n",
            "Episode 1340 Mean Rewards 2.45 \n",
            "Episode 1350 Mean Rewards 2.46 \n",
            "Episode 1360 Mean Rewards 2.41 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3152552. New best score: 2.59\n",
            "Episode 1370 Mean Rewards 3.04 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3179093. New best score: 3.56\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3195216. New best score: 4.41\n",
            "Episode 1380 Mean Rewards 4.63 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3209997. New best score: 5.22\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3225127. New best score: 6.19\n",
            "Episode 1390 Mean Rewards 6.19 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3243322. New best score: 7.14\n",
            "Episode 1400 Mean Rewards 7.54 \n",
            "Total steps so far: 3250584\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3261142. New best score: 7.96\n",
            "Episode 1410 Mean Rewards 8.65 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3282367. New best score: 8.82\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3302832. New best score: 9.66\n",
            "Episode 1420 Mean Rewards 9.79 \n",
            "Episode 1430 Mean Rewards 10.04 \n",
            "Episode 1440 Mean Rewards 10.25 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3389057. New best score: 10.59\n",
            "Episode 1450 Mean Rewards 10.68 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3446739. New best score: 11.42\n",
            "Episode 1460 Mean Rewards 11.42 \n",
            "Episode 1470 Mean Rewards 11.75 \n",
            "Episode 1480 Mean Rewards 11.98 \n",
            "Episode 1490 Mean Rewards 11.88 \n",
            "Episode 1500 Mean Rewards 11.46 \n",
            "Total steps so far: 3568927\n",
            "Episode 1510 Mean Rewards 11.04 \n",
            "Episode 1520 Mean Rewards 10.82 \n",
            "Episode 1530 Mean Rewards 11.17 \n",
            "Episode 1540 Mean Rewards 11.61 \n",
            "Episode 1550 Mean Rewards 12.22 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3730813. New best score: 12.26\n",
            "Episode 1560 Mean Rewards 12.69 \n",
            "Episode 1570 Mean Rewards 12.90 \n",
            "Episode 1580 Mean Rewards 12.24 \n",
            "Episode 1590 Mean Rewards 12.30 \n",
            "Episode 1600 Mean Rewards 12.37 \n",
            "Total steps so far: 3887329\n",
            "Episode 1610 Mean Rewards 12.68 \n",
            "Episode 1620 Mean Rewards 12.89 \n",
            "Episode 1630 Mean Rewards 12.58 \n",
            "Episode 1640 Mean Rewards 12.34 \n",
            "Episode 1650 Mean Rewards 12.26 \n",
            "Episode 1660 Mean Rewards 11.91 \n",
            "Episode 1670 Mean Rewards 11.16 \n",
            "Episode 1680 Mean Rewards 11.42 \n",
            "Episode 1690 Mean Rewards 10.73 \n",
            "Episode 1700 Mean Rewards 10.42 \n",
            "Total steps so far: 4211868\n",
            "Episode 1710 Mean Rewards 10.06 \n",
            "Episode 1720 Mean Rewards 9.71 \n",
            "Episode 1730 Mean Rewards 10.24 \n",
            "Episode 1740 Mean Rewards 10.32 \n",
            "Episode 1750 Mean Rewards 9.98 \n",
            "Episode 1760 Mean Rewards 10.40 \n",
            "Episode 1770 Mean Rewards 10.94 \n",
            "Episode 1780 Mean Rewards 10.92 \n",
            "Episode 1790 Mean Rewards 10.94 \n",
            "Episode 1800 Mean Rewards 11.22 \n",
            "Total steps so far: 4526168\n",
            "Episode 1810 Mean Rewards 11.51 \n",
            "Episode 1820 Mean Rewards 11.28 \n",
            "Episode 1830 Mean Rewards 10.52 \n",
            "Episode 1840 Mean Rewards 10.83 \n",
            "Episode 1850 Mean Rewards 10.67 \n",
            "Episode 1860 Mean Rewards 10.07 \n",
            "Episode 1870 Mean Rewards 9.98 \n",
            "Episode 1880 Mean Rewards 9.63 \n",
            "Episode 1890 Mean Rewards 9.97 \n",
            "Episode 1900 Mean Rewards 9.92 \n",
            "Total steps so far: 4866416\n",
            "Episode 1910 Mean Rewards 9.70 \n",
            "Episode 1920 Mean Rewards 9.69 \n",
            "Episode 1930 Mean Rewards 10.12 \n",
            "Episode 1940 Mean Rewards 9.41 \n",
            "Episode 1950 Mean Rewards 9.13 \n",
            "Episode 1960 Mean Rewards 9.01 \n",
            "Episode 1970 Mean Rewards 8.99 \n",
            "Episode 1980 Mean Rewards 9.04 \n",
            "Episode 1990 Mean Rewards 8.90 \n",
            "Episode 2000 Mean Rewards 8.58 \n",
            "Total steps so far: 5243120\n",
            "Check point\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Episode 2010 Mean Rewards 8.73 \n",
            "Episode 2020 Mean Rewards 9.66 \n",
            "Episode 2030 Mean Rewards 9.65 \n",
            "Episode 2040 Mean Rewards 10.16 \n",
            "Episode 2050 Mean Rewards 10.73 \n",
            "Episode 2060 Mean Rewards 10.91 \n",
            "Episode 2070 Mean Rewards 11.33 \n",
            "Episode 2080 Mean Rewards 11.64 \n",
            "Episode 2090 Mean Rewards 11.15 \n",
            "Episode 2100 Mean Rewards 11.78 \n",
            "Total steps so far: 5505222\n",
            "Episode 2110 Mean Rewards 11.92 \n",
            "Episode 2120 Mean Rewards 11.70 \n",
            "Episode 2130 Mean Rewards 12.29 \n",
            "Episode 2140 Mean Rewards 12.53 \n",
            "Episode 2150 Mean Rewards 13.04 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 5626003. New best score: 13.37\n",
            "Episode 2160 Mean Rewards 13.71 \n",
            "Episode 2170 Mean Rewards 13.83 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 5667446. New best score: 14.31\n",
            "Episode 2180 Mean Rewards 14.39 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 5685867. New best score: 15.14\n",
            "Episode 2190 Mean Rewards 15.65 \n",
            "Episode 2200 Mean Rewards 15.56 \n",
            "Total steps so far: 5722559\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 5744585. New best score: 15.95\n",
            "Episode 2210 Mean Rewards 15.95 \n",
            "Episode 2220 Mean Rewards 16.14 \n",
            "Episode 2230 Mean Rewards 16.13 \n",
            "Episode 2240 Mean Rewards 16.39 \n",
            "Episode 2250 Mean Rewards 16.37 \n",
            "Episode 2260 Mean Rewards 16.37 \n",
            "Episode 2270 Mean Rewards 16.65 \n",
            "Episode 2280 Mean Rewards 16.40 \n",
            "Episode 2290 Mean Rewards 16.29 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 5929629. New best score: 16.76\n",
            "Episode 2300 Mean Rewards 16.76 \n",
            "Total steps so far: 5929629\n",
            "Episode 2310 Mean Rewards 16.72 \n",
            "Episode 2320 Mean Rewards 16.64 \n",
            "Episode 2330 Mean Rewards 16.71 \n",
            "Episode 2340 Mean Rewards 16.45 \n",
            "Episode 2350 Mean Rewards 16.37 \n",
            "Episode 2360 Mean Rewards 16.04 \n",
            "Episode 2370 Mean Rewards 16.10 \n",
            "Episode 2380 Mean Rewards 16.07 \n",
            "Episode 2390 Mean Rewards 16.20 \n",
            "Episode 2400 Mean Rewards 16.24 \n",
            "Total steps so far: 6136455\n",
            "Episode 2410 Mean Rewards 16.09 \n",
            "Episode 2420 Mean Rewards 16.08 \n",
            "Episode 2430 Mean Rewards 15.60 \n",
            "Episode 2440 Mean Rewards 15.59 \n",
            "Episode 2450 Mean Rewards 15.77 \n",
            "Episode 2460 Mean Rewards 16.05 \n",
            "Episode 2470 Mean Rewards 15.82 \n",
            "Episode 2480 Mean Rewards 15.65 \n",
            "Episode 2490 Mean Rewards 15.25 \n",
            "Episode 2500 Mean Rewards 14.88 \n",
            "Total steps so far: 6375853\n",
            "Episode 2510 Mean Rewards 15.02 \n",
            "Episode 2520 Mean Rewards 14.35 \n",
            "Episode 2530 Mean Rewards 14.39 \n",
            "Episode 2540 Mean Rewards 14.12 \n",
            "Episode 2550 Mean Rewards 14.11 \n",
            "Episode 2560 Mean Rewards 14.21 \n",
            "Episode 2570 Mean Rewards 14.48 \n",
            "Episode 2580 Mean Rewards 14.77 \n",
            "Episode 2590 Mean Rewards 15.10 \n",
            "Episode 2600 Mean Rewards 15.38 \n",
            "Total steps so far: 6594447\n",
            "Episode 2610 Mean Rewards 15.54 \n",
            "Episode 2620 Mean Rewards 16.15 \n",
            "Episode 2630 Mean Rewards 16.22 \n",
            "Episode 2640 Mean Rewards 16.62 \n",
            "Episode 2650 Mean Rewards 15.84 \n",
            "Episode 2660 Mean Rewards 15.73 \n",
            "Episode 2670 Mean Rewards 15.39 \n",
            "Episode 2680 Mean Rewards 15.50 \n",
            "Episode 2690 Mean Rewards 15.54 \n",
            "Episode 2700 Mean Rewards 15.44 \n",
            "Total steps so far: 6811913\n",
            "Episode 2710 Mean Rewards 15.52 \n",
            "Episode 2720 Mean Rewards 15.93 \n",
            "Episode 2730 Mean Rewards 16.16 \n",
            "Episode 2740 Mean Rewards 16.42 \n",
            "Episode 2750 Mean Rewards 17.16 \n",
            "Episode 2760 Mean Rewards 17.27 \n",
            "Episode 2770 Mean Rewards 17.34 \n",
            "Episode 2780 Mean Rewards 17.16 \n",
            "Episode 2790 Mean Rewards 17.22 \n",
            "Episode 2800 Mean Rewards 17.44 \n",
            "Total steps so far: 7013024\n",
            "Episode 2810 Mean Rewards 17.35 \n",
            "Episode 2820 Mean Rewards 16.99 \n",
            "Episode 2830 Mean Rewards 16.91 \n",
            "Episode 2840 Mean Rewards 16.86 \n",
            "Episode 2850 Mean Rewards 16.86 \n",
            "Episode 2860 Mean Rewards 17.08 \n",
            "Episode 2870 Mean Rewards 17.37 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 7165920. New best score: 17.57\n",
            "Episode 2880 Mean Rewards 17.65 \n",
            "Episode 2890 Mean Rewards 17.63 \n",
            "Episode 2900 Mean Rewards 17.56 \n",
            "Total steps so far: 7219509\n",
            "Episode 2910 Mean Rewards 17.74 \n",
            "Episode 2920 Mean Rewards 18.01 \n",
            "Episode 2930 Mean Rewards 18.20 \n",
            "Episode 2940 Mean Rewards 17.85 \n",
            "Episode 2950 Mean Rewards 18.05 \n",
            "Episode 2960 Mean Rewards 17.72 \n",
            "Episode 2970 Mean Rewards 17.56 \n",
            "Episode 2980 Mean Rewards 17.51 \n",
            "Episode 2990 Mean Rewards 17.66 \n",
            "Episode 3000 Mean Rewards 17.64 \n",
            "Total steps so far: 7416994\n",
            "Check point\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Episode 3010 Mean Rewards 17.46 \n",
            "Episode 3020 Mean Rewards 17.26 \n",
            "Episode 3030 Mean Rewards 17.15 \n",
            "Episode 3040 Mean Rewards 17.49 \n",
            "Episode 3050 Mean Rewards 17.32 \n",
            "Episode 3060 Mean Rewards 17.56 \n",
            "Episode 3070 Mean Rewards 17.69 \n",
            "Episode 3080 Mean Rewards 17.74 \n",
            "Episode 3090 Mean Rewards 17.81 \n",
            "Episode 3100 Mean Rewards 17.69 \n",
            "Total steps so far: 7621408\n",
            "Episode 3110 Mean Rewards 17.62 \n",
            "Episode 3120 Mean Rewards 17.24 \n",
            "Episode 3130 Mean Rewards 17.53 \n",
            "Episode 3140 Mean Rewards 17.58 \n",
            "Episode 3150 Mean Rewards 17.71 \n",
            "Episode 3160 Mean Rewards 17.57 \n",
            "Episode 3170 Mean Rewards 17.42 \n",
            "Episode 3180 Mean Rewards 17.28 \n",
            "Episode 3190 Mean Rewards 16.98 \n",
            "Episode 3200 Mean Rewards 17.16 \n",
            "Total steps so far: 7834606\n",
            "Episode 3210 Mean Rewards 17.35 \n",
            "Episode 3220 Mean Rewards 17.87 \n",
            "Episode 3230 Mean Rewards 17.74 \n",
            "Episode 3240 Mean Rewards 17.60 \n",
            "Episode 3250 Mean Rewards 17.62 \n",
            "Episode 3260 Mean Rewards 17.79 \n",
            "Episode 3270 Mean Rewards 17.92 \n",
            "Episode 3280 Mean Rewards 18.23 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 7992588. New best score: 18.38\n",
            "Episode 3290 Mean Rewards 18.41 \n",
            "Episode 3300 Mean Rewards 18.46 \n",
            "Total steps so far: 8025052\n",
            "Episode 3310 Mean Rewards 18.49 \n",
            "Episode 3320 Mean Rewards 18.52 \n",
            "Episode 3330 Mean Rewards 18.59 \n",
            "Episode 3340 Mean Rewards 18.76 \n",
            "Episode 3350 Mean Rewards 18.66 \n",
            "Episode 3360 Mean Rewards 18.71 \n",
            "Episode 3370 Mean Rewards 18.69 \n",
            "Episode 3380 Mean Rewards 18.29 \n",
            "Episode 3390 Mean Rewards 18.34 \n",
            "Episode 3400 Mean Rewards 18.12 \n",
            "Total steps so far: 8217323\n",
            "Episode 3410 Mean Rewards 18.13 \n",
            "Episode 3420 Mean Rewards 18.16 \n",
            "Episode 3430 Mean Rewards 18.02 \n",
            "Episode 3440 Mean Rewards 17.96 \n",
            "Episode 3450 Mean Rewards 17.96 \n",
            "Episode 3460 Mean Rewards 17.74 \n",
            "Episode 3470 Mean Rewards 17.47 \n",
            "Episode 3480 Mean Rewards 17.69 \n",
            "Episode 3490 Mean Rewards 17.79 \n",
            "Episode 3500 Mean Rewards 17.82 \n",
            "Total steps so far: 8414935\n",
            "Episode 3510 Mean Rewards 17.85 \n",
            "Episode 3520 Mean Rewards 17.74 \n",
            "Episode 3530 Mean Rewards 17.93 \n",
            "Episode 3540 Mean Rewards 18.04 \n",
            "Episode 3550 Mean Rewards 18.09 \n",
            "Episode 3560 Mean Rewards 18.24 \n",
            "Episode 3570 Mean Rewards 18.47 \n",
            "Episode 3580 Mean Rewards 18.63 \n",
            "Episode 3590 Mean Rewards 18.66 \n",
            "Episode 3600 Mean Rewards 18.85 \n",
            "Total steps so far: 8605832\n",
            "Episode 3610 Mean Rewards 18.69 \n",
            "Episode 3620 Mean Rewards 18.72 \n",
            "Episode 3630 Mean Rewards 18.62 \n",
            "Episode 3640 Mean Rewards 18.30 \n",
            "Episode 3650 Mean Rewards 18.27 \n",
            "Episode 3660 Mean Rewards 17.91 \n",
            "Episode 3670 Mean Rewards 17.95 \n",
            "Episode 3680 Mean Rewards 17.93 \n",
            "Episode 3690 Mean Rewards 17.14 \n",
            "Episode 3700 Mean Rewards 16.95 \n",
            "Total steps so far: 8808856\n",
            "Episode 3710 Mean Rewards 17.13 \n",
            "Episode 3720 Mean Rewards 17.23 \n",
            "Episode 3730 Mean Rewards 17.28 \n",
            "Episode 3740 Mean Rewards 17.41 \n",
            "Episode 3750 Mean Rewards 17.48 \n",
            "Episode 3760 Mean Rewards 17.58 \n",
            "Episode 3770 Mean Rewards 17.53 \n",
            "Episode 3780 Mean Rewards 17.38 \n",
            "Episode 3790 Mean Rewards 17.71 \n",
            "Episode 3800 Mean Rewards 17.44 \n",
            "Total steps so far: 9007731\n",
            "Episode 3810 Mean Rewards 16.85 \n",
            "Episode 3820 Mean Rewards 16.40 \n",
            "Episode 3830 Mean Rewards 16.17 \n",
            "Episode 3840 Mean Rewards 16.01 \n",
            "Episode 3850 Mean Rewards 15.72 \n",
            "Episode 3860 Mean Rewards 15.54 \n",
            "Episode 3870 Mean Rewards 15.17 \n",
            "Episode 3880 Mean Rewards 15.03 \n"
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "t0 = datetime.now()\n",
        "\n",
        "steps = 0\n",
        "\n",
        "training_rewards = []\n",
        "training_loss = []\n",
        "mean_episode_rewards = []\n",
        "episodes_reward = []\n",
        "episodes_loss = []\n",
        "best_score = -21\n",
        "save_score_target = -19\n",
        "\n",
        "current_episode = len(mean_episode_rewards)\n",
        "# steps = 0\n",
        "\n",
        "# training_loss = episodes_loss\n",
        "# episodes_reward = mean_episode_rewards\n",
        "# save_score_target = best_score + 0.8\n",
        "\n",
        "for i in range(max_episodes):\n",
        "    \n",
        "    done = False\n",
        "    s0 = env.reset()\n",
        "    ep_rewards = 0\n",
        "    ep_losses = []\n",
        "    agent.reset_frames()\n",
        "    \n",
        "    while not done:\n",
        "        steps += 1\n",
        "        \n",
        "        # taking action\n",
        "        s0, reward, done = agent.take_action(s0)\n",
        "        ep_rewards += reward\n",
        "        \n",
        "        # learning\n",
        "        if steps % network_update_frequency == 0:\n",
        "            loss = agent.learn(batch_size)\n",
        "            ep_losses.append(loss)\n",
        "        \n",
        "        # update epsion\n",
        "        if steps % network_epsilon_update_frequency == 0 and steps >= 50000:\n",
        "            agent.update_epsilon()\n",
        "        \n",
        "    if done:    \n",
        "        # updates after end of episode\n",
        "        episodes_reward.append(ep_rewards)\n",
        "        episodes_loss.append(np.mean(ep_losses))\n",
        "\n",
        "        if len(episodes_reward) >= window:\n",
        "            mean_rewards = np.mean(\n",
        "                                episodes_reward[-window:])\n",
        "            mean_episode_rewards.append(mean_rewards)\n",
        "\n",
        "        else:\n",
        "            mean_rewards = np.mean(\n",
        "                                episodes_reward)\n",
        "            mean_episode_rewards.append(mean_rewards)\n",
        "\n",
        "\n",
        "        if mean_rewards > best_score:\n",
        "            best_score = mean_rewards\n",
        "            if mean_rewards > save_score_target:\n",
        "                    save_score_target = mean_rewards + 0.8\n",
        "                    save_data(i+current_episode, int(100*mean_rewards))\n",
        "                    print(f'Model Saved. Total Steps: {steps}. New best score: {mean_rewards}')\n",
        "                \n",
        "        if i % 10 == 0:\n",
        "            print(\"Episode {:d} Mean Rewards {:.2f} \".format(\n",
        "                        i+current_episode, mean_rewards))\n",
        "        if i % 100 ==0:\n",
        "            print('Total steps so far:', steps)\n",
        "            if agent.epsilon > agent.epsilon_final:\n",
        "                print('Current epsilon:', agent.epsilon)\n",
        "        \n",
        "        if i % 1000 ==0 and i>0:\n",
        "            print(\"Check point\")\n",
        "            save_data(i+current_episode, int(100*mean_rewards))\n",
        "\n",
        "\n",
        "        if mean_rewards >= rewards_target:\n",
        "            print(f\"Environment Solved after {i} episodes and {steps} steps\")\n",
        "            break\n",
        "        \n",
        "\n",
        "            \n",
        "        \n",
        "        \n",
        "dt = (datetime.now() - t0)\n",
        "print('Script run in:', dt, 'seconds')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "Pufl5NgReR1z",
        "outputId": "0ae83239-eeff-45f9-bb2d-24a3bfd5c5aa"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(mean_rewards)\n",
        "#plt.legend()\n",
        "plt.title(\"Pong Dueling Q-Network Mean rewards \")\n",
        "plt.xlabel('frames')#, fontsize=18)\n",
        "plt.ylabel('mean rewards')#, fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "zgV7MHFheVOl",
        "outputId": "6326d608-301f-422a-cb2b-c207329e0e7e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#plt.plot(episodes_reward, label='episode rewards')\n",
        "plt.plot(episodes_loss)\n",
        "plt.title(\"Pong Dueling Q-Network losses \")\n",
        "plt.xlabel('frames')#, fontsize=18)\n",
        "plt.ylabel('losses')#, fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2j0gkOFYhQu8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Pong Dueling DQN-Submit.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}