{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "_5Y1ILo-c63k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU_5-BKk8tzG"
      },
      "outputs": [],
      "source": [
        "!pip install gym\\[atari,accept-rom-license\\]==0.21.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU/Memory Info"
      ],
      "metadata": {
        "id": "7Mym-Qhac9-h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVuE1HHDDObv",
        "outputId": "5713a308-9113-4d01-a453-417bd19c1d7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Apr 14 09:33:30 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ke4ew9MGDPEJ",
        "outputId": "067f729b-c63d-4fb6-eaf3-9bb6d9132c46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "MIATqYEudBsX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "h0M_2qhJ7Skl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import resize\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import os, dill\n",
        "import random, time\n",
        "import gym\n",
        "\n",
        "from collections import namedtuple, deque\n",
        "from copy import deepcopy, copy\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv8GkXFgSP8y",
        "outputId": "ee6102bc-b942-479f-9bbb-56abdffcb7d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbwemBiS72PA",
        "outputId": "ab572c64-be7e-4175-c303-0b4d2e9ad345"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrapper class for Pong\n",
        "\n",
        "[source](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/wrappers.py)"
      ],
      "metadata": {
        "id": "VkjM-uc0dF91"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N67PnOzkkImh"
      },
      "outputs": [],
      "source": [
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        return obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh3uCEZt78K4"
      },
      "source": [
        "# Explore environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "T3pIm8NV780w",
        "outputId": "d6b27f73-ebc9-4845-9554-548ab71664ad"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = gym.make(\"PongDeterministic-v4\")\n",
        "env = FireResetEnv(env)\n",
        "state = env.reset()\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.imshow(state)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdGp1Tlw9pe3",
        "outputId": "42901929-1599-49f1-e1d2-fdaaeb2a072f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of possible actions: 6\n"
          ]
        }
      ],
      "source": [
        "print(f'Number of possible actions: {env.action_space.n}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMn3waeav1IK"
      },
      "outputs": [],
      "source": [
        "def play_one_episode():\n",
        "    done = False\n",
        "    s = env.reset()\n",
        "    reward = 0\n",
        "    while not done:\n",
        "        s, r, done, _ = env.step(env.action_space.sample())\n",
        "        reward += r\n",
        "\n",
        "    return reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOjezIsU-BhN"
      },
      "source": [
        "# Preprocessing:\n",
        "\n",
        "1. Convert to grayscale.\n",
        "2. Ressize to 75 x 80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE72hyAe-JlZ"
      },
      "outputs": [],
      "source": [
        "# # convert to gray scale\n",
        "# def convert_to_gray(img):\n",
        "#     return np.dot(img, [0.2989, 0.5870, 0.1140])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[source](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)"
      ],
      "metadata": {
        "id": "E2aQ6OhddOdO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NkfOJ__kOU2"
      },
      "outputs": [],
      "source": [
        "def convert_to_gray_rescale(img):\n",
        "    \"\"\" prepro 210x160x3 uint8 frame into 6000 (75x80) 1D float vector \"\"\"\n",
        "    img = img[35:185] # crop - remove 35px from start & 25px from end of image in x, to reduce redundant parts of image (i.e. after ball passes paddle)\n",
        "    img = img[::2,::2,0] # downsample by factor of 2.\n",
        "    img[img == 144] = 0 # erase background (background type 1)\n",
        "    img[img == 109] = 0 # erase background (background type 2)\n",
        "    img[img != 0] = 1 # everything else (paddles, ball) just set to 1. this makes the image grayscale effectively\n",
        "    return img "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "t5YIepNL_GbB",
        "outputId": "1d51b467-b4f1-4e0d-87ff-7cb8f710af14"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "state_g = convert_to_gray_rescale(state)\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.imshow(state_g, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGKxFBjHx3zR"
      },
      "outputs": [],
      "source": [
        "def normalize(img):\n",
        "    return img / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrReW_0MAHr_"
      },
      "outputs": [],
      "source": [
        "# putting everything together\n",
        "def preprocess(img):\n",
        "    img_g = convert_to_gray_rescale(img)\n",
        "    #img_t = resize(img_g, (84, 84))\n",
        "    img_n = normalize(img_g)\n",
        "    return img_n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replay Buffer Class"
      ],
      "metadata": {
        "id": "rhxcjRV7dTMO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU6rtZXwAq4J"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, env, capacity, n_frames):\n",
        "        self.env = env\n",
        "        self.capacity = capacity\n",
        "        self.n_frames = n_frames\n",
        "        self.buffer = namedtuple('Buffer', field_names=['state', 'action', 'reward', 'done','next_state'])\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "        \n",
        "    def push(self, state, action, reward, done, next_state):\n",
        "        # convert (state, action, next_state, reward, done)\n",
        "        self.memory.append(self.buffer(state, action, reward, done, next_state))\n",
        "        \n",
        "        \n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.memory), batch_size, replace=False)\n",
        "        batch = zip(*[self.memory[i] for i in indices])\n",
        "        return batch\n",
        "    \n",
        "    def populate(self, length):\n",
        "        # populate buffer\n",
        "        state_frames = deque(maxlen=self.n_frames)\n",
        "        next_state_frames = deque(maxlen=self.n_frames)\n",
        "        zeros = np.zeros((75,80))\n",
        "\n",
        "\n",
        "        while len(self.memory) < length:\n",
        "            # init frames\n",
        "            for i in range(self.n_frames):\n",
        "                state_frames.append(zeros)\n",
        "                next_state_frames.append(zeros)\n",
        "            \n",
        "            done = False\n",
        "            s0 = self.env.reset()\n",
        "            while not done:\n",
        "                action = self.env.action_space.sample()\n",
        "                s1, r, done, _ = self.env.step(action)\n",
        "\n",
        "                # build input states\n",
        "                s0_t = preprocess(s0) # (84, 84)\n",
        "                state_frames.append(s0_t) # stack of 4 (84,84)\n",
        "                s1_t = preprocess(s1) # (84, 84)\n",
        "                next_state_frames.append(s1_t) # stack of 4 (84,84)\n",
        "\n",
        "                # stack 4 frames in  tensor\n",
        "                input_state = np.stack(state_frames)\n",
        "                input_next_state = np.stack(next_state_frames)\n",
        "\n",
        "                # update buffer\n",
        "                self.push(input_state, action, r, done, input_next_state)\n",
        "                s0 = s1\n",
        "\n",
        "                if len(self.memory) >= length:\n",
        "                    break\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Double Q-Network Class"
      ],
      "metadata": {
        "id": "Z1JG-fTGdWor"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GVqgc3tNQF4Z"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self,env, n_frames):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(n_frames, 32, kernel_size=8, stride=4), # input_shape: batch_size x 4 x 84 x 84 -> batch_size x 32 x 20 x 20\n",
        "            #nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            #nn.BatchNorm2d(64),\n",
        "            nn.ReLU(), \n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            #nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            )\n",
        "        \n",
        "        conv_out_shape = self._get_input_shape([4,75,80]) # \n",
        "\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_shape, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, env.action_space.n)\n",
        "        )\n",
        "              \n",
        "    def forward(self, x):\n",
        "        # conv layer output\n",
        "        features = self.conv(x)\n",
        "        features = features.view(x.size(0), -1)\n",
        "\n",
        "        q_values = self.fc(features)\n",
        "\n",
        "        return q_values\n",
        "        \n",
        "    def _get_input_shape(self, shape):\n",
        "        conv_out = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(conv_out.size()))\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent Class"
      ],
      "metadata": {
        "id": "zu-cwXSFdkkQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73c5BoOMTR_E"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self, env, buffer, epsilon, epsilon_mid, epsilon_final, decay_mid, decay_final):\n",
        "        self.env = env\n",
        "        self.buffer = buffer\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_start = epsilon\n",
        "        self.epsilon_mid = epsilon_mid\n",
        "        self.epsilon_final = epsilon_final\n",
        "        self.decay_mid = decay_mid\n",
        "        self.decay_final = decay_final\n",
        "        self.state_frames = deque(maxlen=buffer.n_frames)\n",
        "        self.next_state_frames = deque(maxlen=buffer.n_frames)\n",
        "        self.reset_frames()\n",
        "        \n",
        "    def take_action(self, state):\n",
        "        \n",
        "        # build state frames\n",
        "        state_frames = self.build_state_frames(state) # 4, 75, 80\n",
        "        # which action to take ?\n",
        "        action = self.get_action(state_frames)\n",
        "\n",
        "        # take the action\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "        # build next state frames\n",
        "        next_state_frames = self.build_next_state_frames(next_state)\n",
        "\n",
        "        # push new buffer\n",
        "        self.buffer.push(state_frames, action, reward, done, next_state_frames)\n",
        "        \n",
        "        return next_state, reward, done\n",
        "        \n",
        "    def get_action(self, state_frames):\n",
        "        if random.random() < self.epsilon:\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            action = self.get_greedy_action(state_frames)\n",
        "        return action\n",
        "            \n",
        "    def get_greedy_action(self, state_frames):\n",
        "        with torch.no_grad():\n",
        "            Q_network.eval()\n",
        "            state_frames = np.expand_dims(state_frames, 0)\n",
        "            state_frames = torch.FloatTensor(state_frames).to(device)\n",
        "            index_action = torch.argmax(Q_network(state_frames).cpu()).item()\n",
        "            Q_network.train()\n",
        "        return index_action\n",
        "    \n",
        "    \n",
        "    def learn(self, batch_size):\n",
        "        # sample a batch\n",
        "        batch = self.buffer.sample(batch_size)\n",
        "        states, actions, rewards, dones, next_states = [i for i in batch]\n",
        "\n",
        "        # transform batch to tensor then to device\n",
        "        states_t = torch.FloatTensor(np.array(states)).to(device)\n",
        "        actions_t = torch.LongTensor(np.array(actions)).to(device)\n",
        "        rewards_t = torch.FloatTensor(np.array(rewards)).to(device)\n",
        "        dones_t = torch.BoolTensor(np.array(dones)).to(device)\n",
        "        next_states_t = torch.FloatTensor(np.array(next_states)).to(device)\n",
        "\n",
        "        # update rule : q(s,a) = q(s,a) + (r + gamma * max_a'_q(s',a') - q(s,a))\n",
        "        # y = r + gamma * max_a'_q(s',a')\n",
        "        # get q_vals target\n",
        "        next_q_values = Q_target(next_states_t).detach().max(1)[0]# batch size, 1\n",
        "        next_q_values[dones_t] = 0 # zero the final states\n",
        "        y = rewards_t + gamma * next_q_values\n",
        "        y = y.unsqueeze(1)\n",
        "    \n",
        "        # q_values\n",
        "        q_values = Q_network(states_t).gather(1, actions_t.reshape(-1, 1))\n",
        "        \n",
        "        # compute loss\n",
        "        loss = criterion(q_values, y)\n",
        "        \n",
        "        # back prop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # # Trick: gradient clipping\n",
        "        for param in Q_network.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "\n",
        "        # gradient descent\n",
        "        optimizer.step()\n",
        "        \n",
        "        return loss.item()\n",
        "         \n",
        "    def reset_frames(self):\n",
        "        zeros = np.zeros((75,80))\n",
        "        for i in range(self.buffer.n_frames):\n",
        "            self.state_frames.append(zeros)\n",
        "            self.next_state_frames.append(zeros)\n",
        "\n",
        "    def build_state_frames(self, state):\n",
        "        state_t = preprocess(state)\n",
        "        self.state_frames.append(state_t)\n",
        "        return np.stack(self.state_frames)\n",
        "\n",
        "    def build_next_state_frames(self, next_state):\n",
        "        next_state_t = preprocess(next_state)\n",
        "        self.next_state_frames.append(next_state_t)\n",
        "        return np.stack(self.next_state_frames)\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        e = self.epsilon \n",
        "\n",
        "        if e > self.epsilon_mid:\n",
        "            self.epsilon = max(self.epsilon_mid, e - self.decay_mid)\n",
        "        else:\n",
        "            self.epsilon = max(self.epsilon_final, e - self.decay_final)\n",
        " \n",
        "    def reset_epsilon(self):\n",
        "        self.epsilon = self.epsilon_start"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utiliy for copying Q_net to Q_target"
      ],
      "metadata": {
        "id": "QQAYb2iIdolG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_WnTPusRe4F"
      },
      "outputs": [],
      "source": [
        "def copy_network(Q_network, Q_target):\n",
        "    Q_target.load_state_dict(Q_network.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standard Weight Initialization function"
      ],
      "metadata": {
        "id": "J8ky56wbdulV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBEf6oAZiv8p"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(m):\n",
        "  if isinstance(m, nn.Conv2d):\n",
        "      nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
        "      if m.bias is not None:\n",
        "          nn.init.constant_(m.bias.data, 0)\n",
        "#   elif isinstance(m, nn.BatchNorm2d):\n",
        "#       nn.init.constant_(m.weight.data, 1)\n",
        "#       nn.init.constant_(m.bias.data, 0)\n",
        "  elif isinstance(m, nn.Linear):\n",
        "      nn.init.kaiming_uniform_(m.weight.data)\n",
        "      nn.init.constant_(m.bias.data, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "OTeot5IddzrJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z79ssfZ1kFcE"
      },
      "outputs": [],
      "source": [
        "# Hyper params \n",
        "capacity = 40000\n",
        "n_frames = 4\n",
        "epsilon = 1\n",
        "epsilon_mid = 0.1\n",
        "epsilon_final = 0.02\n",
        "initial_buffer_size = 10000\n",
        "mid_exploration_frame = 1000000\n",
        "final_exploration_frame = 2400000\n",
        "\n",
        "decay_mid =  (epsilon - epsilon_mid) / mid_exploration_frame\n",
        "\n",
        "decay_final = (epsilon_mid - epsilon_final) / (final_exploration_frame - mid_exploration_frame)\n",
        "\n",
        "\n",
        "lr = 1e-4 #25e-5 # ok for pong\n",
        "batch_size = 32\n",
        "\n",
        "max_episodes = 5000\n",
        "rewards_target = 19\n",
        "window = 100\n",
        "network_update_frequency = 4\n",
        "network_sync_frequency = 1000\n",
        "network_epsilon_update_frequency = 1\n",
        "gamma = 0.99\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instatiate and Populate Replay Buffer"
      ],
      "metadata": {
        "id": "VHtzFVkyd1I_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW9vFtEykR6Q",
        "outputId": "93552e67-d47d-40b0-b953-1aa34507533b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It took : 0:00:09.502707 to populate buffer of size: 10000\n"
          ]
        }
      ],
      "source": [
        "buffer = ReplayBuffer(env, capacity, n_frames)\n",
        "t0 = datetime.now()\n",
        "buffer.populate(initial_buffer_size)\n",
        "dt = datetime.now() - t0\n",
        "\n",
        "print(\"It took :\", dt, \"to populate buffer of size:\", len(buffer))\n",
        "agent = Agent(env, buffer, epsilon, epsilon_mid, epsilon_final, decay_mid, decay_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enfzqHLor0jN"
      },
      "source": [
        "# Save / Load functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BQHFrTTrz1Z"
      },
      "outputs": [],
      "source": [
        "drive_path = \"/content/drive/MyDrive/PongDeterministic-v4-DDQN\"\n",
        "\n",
        "if not os.path.exists(drive_path):\n",
        "    os.mkdir(drive_path)\n",
        "data_names = ['models', 'mean_rewards', 'best_score', 'losses']\n",
        "def save_data(episode, reward):\n",
        "\n",
        "    print(\"saving to drive\")\n",
        "\n",
        "    for data in data_names:\n",
        "        data_path = os.path.join(drive_path, data)\n",
        "        if not os.path.exists(data_path):\n",
        "            os.mkdir(data_path)\n",
        "\n",
        "        if data == 'models':\n",
        "\n",
        "            data_file_name = './pong_ep_{}_score_{}.pth'.format(episode, reward)\n",
        "            data_file_path = os.path.join(data_path, data_file_name)\n",
        "\n",
        "            torch.save(Q_network, data_file_path)\n",
        "            print('model saved')        \n",
        "\n",
        "        else:\n",
        "            print(\"Saving \" + data )\n",
        "            data_file_name = data + f\"_{episode}_{reward}.pkl\"\n",
        "            data_file_path = os.path.join(data_path, data_file_name)\n",
        "\n",
        "            with open(data_file_path, 'wb') as f:\n",
        "                if data == 'mean_rewards':\n",
        "                    dill.dump(mean_episode_rewards, f)\n",
        "                elif data == 'best_score':\n",
        "                    dill.dump(best_score, f)\n",
        "                elif data == 'losses':\n",
        "                    dill.dump(episodes_loss, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wle85688r6TZ"
      },
      "outputs": [],
      "source": [
        "def load_data(episode, reward):\n",
        "\n",
        "    print(\"loading from drive\")\n",
        "\n",
        "    for data in data_names:\n",
        "        data_path = os.path.join(drive_path, data)\n",
        "\n",
        "        if data == 'models':\n",
        "            data_file_name = f\"pong_ep_{episode}_score_{reward}.pth\"\n",
        "            data_file_path = os.path.join(data_path, data_file_name)\n",
        "\n",
        "            Q_network = torch.load(data_file_path, map_location=device)\n",
        "            Q_target = torch.load(data_file_path, map_location=device)\n",
        "            print(\"Models Loaded\")\n",
        "            print(Q_network)\n",
        "        \n",
        "        else:\n",
        "            print(\"Loading \" + data)\n",
        "            data_file_name = data + f\"_{episode}_{reward}.pkl\"\n",
        "            data_file_path = os.path.join(data_path, data_file_name)\n",
        "            with open(data_file_path, 'rb') as f:\n",
        "                if data == 'mean_rewards':\n",
        "                    mean_episode_rewards = dill.load(f)\n",
        "                    print(\"Rewards loaded, length of rewards:\", len(mean_episode_rewards))\n",
        "                elif data == 'best_score':\n",
        "                    best_score = dill.load(f)\n",
        "                    print(\"best score loaded:\", best_score)\n",
        "                elif data == 'losses':\n",
        "                    episodes_loss = dill.load(f)\n",
        "                    print(\"episodes_loss loaded, length of losses:\", len(episodes_loss))\n",
        "\n",
        "    return Q_network, Q_target, mean_episode_rewards, best_score, episodes_loss\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate Networks"
      ],
      "metadata": {
        "id": "WpD36Ogod72-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnvkRiYKZE0L",
        "outputId": "58714aa2-5c5d-4654-db4b-fa0492849692"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QNetwork(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
              "    (3): ReLU()\n",
              "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (5): ReLU()\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=1920, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Q_network = QNetwork(env, n_frames)\n",
        "Q_network.apply(initialize_weights)\n",
        "Q_target = QNetwork(env, n_frames)\n",
        "Q_target.load_state_dict(Q_network.state_dict())\n",
        "Q_target.eval()\n",
        "Q_network.to(device)\n",
        "Q_target.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WwGf5iU2wxa",
        "outputId": "cbf6942e-00d0-48d6-db67-eb382804879f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pong_ep_1000_score_-1697.pth  pong_ep_1587_score_-1223.pth\n",
            "pong_ep_1023_score_-1652.pth  pong_ep_1600_score_-1135.pth\n",
            "pong_ep_1041_score_-1570.pth  pong_ep_629_score_-1898.pth\n",
            "pong_ep_1053_score_-1477.pth  pong_ep_736_score_-1818.pth\n",
            "pong_ep_1076_score_-1387.pth  pong_ep_964_score_-1737.pth\n",
            "pong_ep_1120_score_-1305.pth\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/PongDeterministic-v4-DDQN/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3cU5lRO2u3q",
        "outputId": "66daa8f6-1021-4337-b957-94c1cdd2e07a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading from drive\n",
            "Models Loaded\n",
            "QNetwork(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=1920, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
            "  )\n",
            ")\n",
            "Loading mean_rewards\n",
            "Rewards loaded, length of rewards: 1601\n",
            "Loading best_score\n",
            "best score loaded: -11.35\n",
            "Loading losses\n",
            "episodes_loss loaded, length of losses: 1601\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "QNetwork(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
              "    (3): ReLU()\n",
              "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (5): ReLU()\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=1920, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# In case of loading model\n",
        "# Q_network, Q_target, mean_episode_rewards, best_score, episodes_loss = load_data(1600,-1135)\n",
        "# Q_target.eval()\n",
        "# Q_network.to(device)\n",
        "# Q_target.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "UPo1iQuGeDzV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDhuZX17eCWA"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(Q_network.parameters(), lr=lr)\n",
        "#optimizer = torch.optim.RMSprop(Q_network.parameters(), lr=lr)\n",
        "criterion = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujsong_IZGP1",
        "outputId": "89a6e355-6da2-4f19-ae09-94617428f354"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1601 Mean Rewards -15.03 \n",
            "Total steps so far: 2049197\n",
            "Current epsilon: 0.039730457142868096\n",
            "Episode 1611 Mean Rewards -14.43 \n",
            "Episode 1621 Mean Rewards -13.58 \n",
            "Episode 1631 Mean Rewards -13.22 \n",
            "Episode 1641 Mean Rewards -12.99 \n",
            "Episode 1651 Mean Rewards -13.46 \n",
            "Episode 1661 Mean Rewards -12.69 \n",
            "Episode 1671 Mean Rewards -12.09 \n",
            "Episode 1681 Mean Rewards -11.52 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2303759. New best score: -10.530482827716371\n",
            "Episode 1691 Mean Rewards -9.87 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2320383. New best score: -9.710899191200893\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2337315. New best score: -8.783023532398875\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2357157. New best score: -7.935660805636264\n",
            "Episode 1701 Mean Rewards -7.94 \n",
            "Total steps so far: 2357157\n",
            "Current epsilon: 0.022132742857315306\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2378408. New best score: -7.088049810192452\n",
            "Episode 1711 Mean Rewards -6.91 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2410937. New best score: -6.206771934202583\n",
            "Episode 1721 Mean Rewards -5.43 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2437980. New best score: -5.205033696276011\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2461659. New best score: -4.371276513035969\n",
            "Episode 1731 Mean Rewards -3.76 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2488927. New best score: -3.5639532757590473\n",
            "Episode 1741 Mean Rewards -3.95 \n",
            "Episode 1751 Mean Rewards -3.66 \n",
            "Episode 1761 Mean Rewards -3.31 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2660133. New best score: -2.6197879473988546\n",
            "Episode 1771 Mean Rewards -2.62 \n",
            "Episode 1781 Mean Rewards -1.89 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2705569. New best score: -1.7280552331486032\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2724287. New best score: -0.8385864457564388\n",
            "Episode 1791 Mean Rewards -0.08 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2743830. New best score: 0.11102959579127264\n",
            "Episode 1801 Mean Rewards 0.62 \n",
            "Total steps so far: 2772548\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2781667. New best score: 1.1028901333208323\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2804199. New best score: 1.9145861810402838\n",
            "Episode 1811 Mean Rewards 1.91 \n",
            "Episode 1821 Mean Rewards 2.19 \n",
            "Episode 1831 Mean Rewards 2.10 \n",
            "Episode 1841 Mean Rewards 2.38 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2929786. New best score: 2.7423497583964918\n",
            "Episode 1851 Mean Rewards 2.82 \n",
            "Episode 1861 Mean Rewards 2.99 \n",
            "Episode 1871 Mean Rewards 3.16 \n",
            "Episode 1881 Mean Rewards 3.39 \n",
            "Episode 1891 Mean Rewards 2.61 \n",
            "Episode 1901 Mean Rewards 2.68 \n",
            "Total steps so far: 3082691\n",
            "Episode 1911 Mean Rewards 2.82 \n",
            "Episode 1921 Mean Rewards 1.93 \n",
            "Episode 1931 Mean Rewards 1.93 \n",
            "Episode 1941 Mean Rewards 0.92 \n",
            "Episode 1951 Mean Rewards 0.68 \n",
            "Episode 1961 Mean Rewards -0.35 \n",
            "Episode 1971 Mean Rewards 0.28 \n",
            "Episode 1981 Mean Rewards 0.72 \n",
            "Episode 1991 Mean Rewards 2.19 \n",
            "Episode 2001 Mean Rewards 2.71 \n",
            "Total steps so far: 3384406\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3419278. New best score: 3.6227492907725516\n",
            "Episode 2011 Mean Rewards 3.62 \n",
            "Episode 2021 Mean Rewards 4.23 \n",
            "Episode 2031 Mean Rewards 4.05 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3508865. New best score: 4.476097449662411\n",
            "Episode 2041 Mean Rewards 4.18 \n",
            "Episode 2051 Mean Rewards 3.27 \n",
            "Episode 2061 Mean Rewards 3.17 \n",
            "Episode 2071 Mean Rewards 3.05 \n",
            "Episode 2081 Mean Rewards 1.84 \n",
            "Episode 2091 Mean Rewards 0.11 \n",
            "Episode 2101 Mean Rewards -0.42 \n",
            "Total steps so far: 3685973\n",
            "Episode 2111 Mean Rewards -0.61 \n",
            "Episode 2121 Mean Rewards -2.14 \n",
            "Episode 2131 Mean Rewards -1.17 \n",
            "Episode 2141 Mean Rewards -0.97 \n",
            "Episode 2151 Mean Rewards -0.15 \n",
            "Episode 2161 Mean Rewards 0.71 \n",
            "Episode 2171 Mean Rewards 2.06 \n",
            "Episode 2181 Mean Rewards 1.86 \n",
            "Episode 2191 Mean Rewards 3.41 \n",
            "Episode 2201 Mean Rewards 4.01 \n",
            "Total steps so far: 4003424\n",
            "Episode 2211 Mean Rewards 3.87 \n",
            "Episode 2221 Mean Rewards 2.79 \n",
            "Episode 2231 Mean Rewards 3.93 \n",
            "Episode 2241 Mean Rewards 4.05 \n",
            "Episode 2251 Mean Rewards 4.66 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 4160673. New best score: 5.415002770287222\n",
            "Episode 2261 Mean Rewards 5.60 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 4171072. New best score: 6.257939982355409\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 4181808. New best score: 7.157348749902617\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 4191096. New best score: 8.100503289196036\n",
            "Episode 2271 Mean Rewards 8.10 \n",
            "Episode 2281 Mean Rewards 8.59 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 4223639. New best score: 9.193684202319277\n",
            "Episode 2291 Mean Rewards 9.80 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 4240416. New best score: 10.002691460515901\n",
            "Episode 2301 Mean Rewards 10.80 \n",
            "Total steps so far: 4259127\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 4260942. New best score: 10.922881582630454\n",
            "Episode 2311 Mean Rewards 11.15 \n",
            "Episode 2321 Mean Rewards 10.80 \n",
            "Episode 2331 Mean Rewards 9.95 \n",
            "Episode 2341 Mean Rewards 9.68 \n",
            "Episode 2351 Mean Rewards 9.27 \n",
            "Episode 2361 Mean Rewards 8.50 \n",
            "Episode 2371 Mean Rewards 8.50 \n",
            "Episode 2381 Mean Rewards 9.24 \n",
            "Episode 2391 Mean Rewards 9.48 \n",
            "Episode 2401 Mean Rewards 9.27 \n",
            "Total steps so far: 4563124\n",
            "Episode 2411 Mean Rewards 9.63 \n",
            "Episode 2421 Mean Rewards 9.62 \n",
            "Episode 2431 Mean Rewards 10.21 \n",
            "Episode 2441 Mean Rewards 9.63 \n",
            "Episode 2451 Mean Rewards 9.79 \n",
            "Episode 2461 Mean Rewards 9.90 \n",
            "Episode 2471 Mean Rewards 10.27 \n",
            "Episode 2481 Mean Rewards 10.06 \n",
            "Episode 2491 Mean Rewards 10.43 \n",
            "Episode 2501 Mean Rewards 10.71 \n",
            "Total steps so far: 4912510\n",
            "Episode 2511 Mean Rewards 10.31 \n",
            "Episode 2521 Mean Rewards 10.25 \n",
            "Episode 2531 Mean Rewards 10.15 \n",
            "Episode 2541 Mean Rewards 10.04 \n",
            "Episode 2551 Mean Rewards 9.78 \n",
            "Episode 2561 Mean Rewards 10.21 \n",
            "Episode 2571 Mean Rewards 10.24 \n",
            "Episode 2581 Mean Rewards 10.66 \n",
            "Episode 2591 Mean Rewards 11.06 \n",
            "Episode 2601 Mean Rewards 11.01 \n",
            "Total steps so far: 5255355\n",
            "Check point\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Episode 2611 Mean Rewards 11.34 \n",
            "Episode 2621 Mean Rewards 11.65 \n",
            "Episode 2631 Mean Rewards 11.26 \n",
            "Episode 2641 Mean Rewards 11.30 \n",
            "Episode 2651 Mean Rewards 11.71 \n",
            "Episode 2661 Mean Rewards 11.63 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 5464283. New best score: 11.80602427509374\n",
            "Episode 2671 Mean Rewards 11.98 \n",
            "Episode 2681 Mean Rewards 12.28 \n",
            "Episode 2691 Mean Rewards 11.86 \n",
            "Episode 2701 Mean Rewards 11.53 \n",
            "Total steps so far: 5585867\n",
            "Episode 2711 Mean Rewards 11.73 \n",
            "Episode 2721 Mean Rewards 10.67 \n",
            "Episode 2731 Mean Rewards 10.07 \n",
            "Episode 2741 Mean Rewards 10.16 \n",
            "Episode 2751 Mean Rewards 10.11 \n",
            "Episode 2761 Mean Rewards 9.69 \n",
            "Episode 2771 Mean Rewards 9.86 \n",
            "Episode 2781 Mean Rewards 9.97 \n",
            "Episode 2791 Mean Rewards 10.13 \n",
            "Episode 2801 Mean Rewards 9.57 \n",
            "Total steps so far: 5968392\n",
            "Episode 2811 Mean Rewards 9.19 \n",
            "Episode 2821 Mean Rewards 8.31 \n",
            "Episode 2831 Mean Rewards 6.99 \n",
            "Episode 2841 Mean Rewards 4.66 \n",
            "Episode 2851 Mean Rewards 2.04 \n",
            "Episode 2861 Mean Rewards -1.35 \n",
            "Episode 2871 Mean Rewards -3.86 \n",
            "Episode 2881 Mean Rewards -5.76 \n",
            "Episode 2891 Mean Rewards -7.59 \n",
            "Episode 2901 Mean Rewards -7.46 \n",
            "Total steps so far: 6211408\n",
            "Episode 2911 Mean Rewards -7.07 \n",
            "Episode 2921 Mean Rewards -6.79 \n",
            "Episode 2931 Mean Rewards -6.42 \n",
            "Episode 2941 Mean Rewards -4.05 \n",
            "Episode 2951 Mean Rewards -2.65 \n",
            "Episode 2961 Mean Rewards -1.16 \n",
            "Episode 2971 Mean Rewards 0.68 \n",
            "Episode 2981 Mean Rewards 2.17 \n",
            "Episode 2991 Mean Rewards 3.14 \n",
            "Episode 3001 Mean Rewards 4.29 \n",
            "Total steps so far: 6524391\n",
            "Episode 3011 Mean Rewards 5.87 \n",
            "Episode 3021 Mean Rewards 7.19 \n",
            "Episode 3031 Mean Rewards 9.23 \n",
            "Episode 3041 Mean Rewards 10.60 \n",
            "Episode 3051 Mean Rewards 11.82 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 6677938. New best score: 12.658969195378845\n",
            "Episode 3061 Mean Rewards 12.72 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 6707877. New best score: 13.548994101889928\n",
            "Episode 3071 Mean Rewards 13.71 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 6734361. New best score: 14.354670650999523\n",
            "Episode 3081 Mean Rewards 14.40 \n",
            "Episode 3091 Mean Rewards 14.68 \n",
            "Episode 3101 Mean Rewards 15.10 \n",
            "Total steps so far: 6787410\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 6789791. New best score: 15.189717070216917\n",
            "Episode 3111 Mean Rewards 15.74 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 6828062. New best score: 15.991484512198731\n",
            "Episode 3121 Mean Rewards 16.05 \n",
            "Episode 3131 Mean Rewards 16.21 \n",
            "Episode 3141 Mean Rewards 16.77 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 6882998. New best score: 16.805764298437097\n",
            "Episode 3151 Mean Rewards 17.15 \n",
            "Episode 3161 Mean Rewards 17.03 \n",
            "Episode 3171 Mean Rewards 17.37 \n",
            "Episode 3181 Mean Rewards 17.47 \n",
            "Episode 3191 Mean Rewards 17.41 \n",
            "Episode 3201 Mean Rewards 17.46 \n",
            "Total steps so far: 7013805\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 7020743. New best score: 17.620204589856893\n",
            "Episode 3211 Mean Rewards 17.69 \n",
            "Episode 3221 Mean Rewards 17.80 \n",
            "Episode 3231 Mean Rewards 17.70 \n",
            "Episode 3241 Mean Rewards 17.75 \n",
            "Episode 3251 Mean Rewards 17.53 \n",
            "Episode 3261 Mean Rewards 17.17 \n",
            "Episode 3271 Mean Rewards 16.67 \n",
            "Episode 3281 Mean Rewards 16.64 \n",
            "Episode 3291 Mean Rewards 16.12 \n",
            "Episode 3301 Mean Rewards 15.83 \n",
            "Total steps so far: 7290349\n",
            "Episode 3311 Mean Rewards 15.80 \n",
            "Episode 3321 Mean Rewards 15.78 \n",
            "Episode 3331 Mean Rewards 15.57 \n",
            "Episode 3341 Mean Rewards 15.91 \n",
            "Episode 3351 Mean Rewards 15.90 \n",
            "Episode 3361 Mean Rewards 15.59 \n",
            "Episode 3371 Mean Rewards 15.09 \n",
            "Episode 3381 Mean Rewards 14.77 \n",
            "Episode 3391 Mean Rewards 14.28 \n",
            "Episode 3401 Mean Rewards 14.09 \n",
            "Total steps so far: 7587988\n",
            "Episode 3411 Mean Rewards 13.14 \n",
            "Episode 3421 Mean Rewards 12.67 \n",
            "Episode 3431 Mean Rewards 12.52 \n",
            "Episode 3441 Mean Rewards 12.07 \n",
            "Episode 3451 Mean Rewards 11.54 \n",
            "Episode 3461 Mean Rewards 12.22 \n",
            "Episode 3471 Mean Rewards 12.97 \n",
            "Episode 3481 Mean Rewards 12.82 \n",
            "Episode 3491 Mean Rewards 12.80 \n",
            "Episode 3501 Mean Rewards 12.36 \n",
            "Total steps so far: 7894267\n",
            "Episode 3511 Mean Rewards 10.73 \n",
            "Episode 3521 Mean Rewards 7.78 \n",
            "Episode 3531 Mean Rewards 5.50 \n",
            "Episode 3541 Mean Rewards 4.52 \n",
            "Episode 3551 Mean Rewards 4.19 \n",
            "Episode 3561 Mean Rewards 5.04 \n",
            "Episode 3571 Mean Rewards 6.54 \n",
            "Episode 3581 Mean Rewards 8.27 \n"
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "t0 = datetime.now()\n",
        "\n",
        "steps = 0\n",
        "\n",
        "training_rewards = []\n",
        "training_loss = []\n",
        "mean_episode_rewards = []\n",
        "episodes_reward = []\n",
        "episodes_loss = []\n",
        "\n",
        "best_score = -21\n",
        "save_score_target = -19\n",
        "\n",
        "current_episode = len(mean_episode_rewards)\n",
        "\n",
        "# In case of loading old model\n",
        "# steps = 2044480\n",
        "\n",
        "# training_loss = episodes_loss\n",
        "# episodes_reward = mean_episode_rewards\n",
        "# save_score_target = best_score + 0.8\n",
        "\n",
        "for i in range(max_episodes):\n",
        "    \n",
        "    done = False\n",
        "    s0 = env.reset()\n",
        "    ep_rewards = 0\n",
        "    ep_losses = []\n",
        "    agent.reset_frames()\n",
        "    \n",
        "    while not done:\n",
        "        steps += 1\n",
        "        \n",
        "        # taking action\n",
        "        s0, reward, done = agent.take_action(s0)\n",
        "        ep_rewards += reward\n",
        "        \n",
        "        # learning\n",
        "        if steps % network_update_frequency == 0:\n",
        "            loss = agent.learn(batch_size)\n",
        "            ep_losses.append(loss)\n",
        "        \n",
        "        # syncing\n",
        "        if steps % network_sync_frequency == 0:\n",
        "            copy_network(Q_network, Q_target)\n",
        "\n",
        "        # update epsion\n",
        "        if steps % network_epsilon_update_frequency == 0 and steps >= 50000:\n",
        "            agent.update_epsilon()\n",
        "        \n",
        "    if done:    \n",
        "        # updates after end of episode\n",
        "        episodes_reward.append(ep_rewards)\n",
        "        episodes_loss.append(np.mean(ep_losses))\n",
        "\n",
        "        if len(episodes_reward) >= window:\n",
        "            mean_rewards = np.mean(\n",
        "                                episodes_reward[-window:])\n",
        "            mean_episode_rewards.append(mean_rewards)\n",
        "\n",
        "        else:\n",
        "            mean_rewards = np.mean(\n",
        "                                episodes_reward)\n",
        "            mean_episode_rewards.append(mean_rewards)\n",
        "\n",
        "\n",
        "        if mean_rewards > best_score:\n",
        "            best_score = mean_rewards\n",
        "            if mean_rewards > save_score_target:\n",
        "                    save_score_target = mean_rewards + 0.8\n",
        "                    save_data(i+current_episode, int(100*mean_rewards))\n",
        "                    print(f'Model Saved. Total Steps: {steps}. New best score: {mean_rewards}')\n",
        "                \n",
        "        if i % 10 == 0:\n",
        "            print(\"Episode {:d} Mean Rewards {:.2f} \".format(\n",
        "                        i+current_episode, mean_rewards))\n",
        "        if i % 100 ==0:\n",
        "            print('Total steps so far:', steps)\n",
        "            if agent.epsilon > agent.epsilon_final:\n",
        "                print('Current epsilon:', agent.epsilon)\n",
        "        \n",
        "        if i % 1000 ==0 and i>0:\n",
        "            print(\"Check point\")\n",
        "            save_data(i+current_episode, int(100*mean_rewards))\n",
        "\n",
        "\n",
        "        if mean_rewards >= rewards_target:\n",
        "            print(f\"Environment Solved after {i} episodes and {steps} steps\")\n",
        "            break\n",
        "        \n",
        "\n",
        "            \n",
        "        \n",
        "        \n",
        "dt = (datetime.now() - t0)\n",
        "print('Script run in:', dt, 'seconds')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUfCIvxRdWJx"
      },
      "outputs": [],
      "source": [
        "print(\"steps:\", steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGcuTNZbnYhi"
      },
      "outputs": [],
      "source": [
        "#plt.plot(episodes_reward, label='episode rewards')\n",
        "plt.plot(mean_episode_rewards[:2000], label='mean rewards')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8cyj5iogcuK"
      },
      "outputs": [],
      "source": [
        "max(mean_episode_rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjIbvFhN32jZ"
      },
      "outputs": [],
      "source": [
        "mean_episode_rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8Xt6S4Ad3tf"
      },
      "outputs": [],
      "source": [
        "N = 2000\n",
        "mean_rewards = mean_episode_rewards.copy()\n",
        "while N < len(mean_episode_rewards):\n",
        "    mean_rewards[N] = np.mean(mean_episode_rewards[N-window:N])\n",
        "    N += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pufl5NgReR1z"
      },
      "outputs": [],
      "source": [
        "plt.plot(mean_rewards)\n",
        "#plt.legend()\n",
        "plt.title(\"Pong Dueling Q-Network Mean rewards \")\n",
        "plt.xlabel('frames')#, fontsize=18)\n",
        "plt.ylabel('mean rewards')#, fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgV7MHFheVOl"
      },
      "outputs": [],
      "source": [
        "#plt.plot(episodes_reward, label='episode rewards')\n",
        "plt.plot(episodes_loss)\n",
        "plt.title(\"Pong Dueling Q-Network losses \")\n",
        "plt.xlabel('frames')#, fontsize=18)\n",
        "plt.ylabel('losses')#, fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2j0gkOFYhQu8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Pong DDQN - Submit.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}