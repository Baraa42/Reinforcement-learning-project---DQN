{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "mYg67bWGebaG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU_5-BKk8tzG"
      },
      "outputs": [],
      "source": [
        "!pip install gym\\[atari,accept-rom-license\\]==0.21.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU Info"
      ],
      "metadata": {
        "id": "hwergxLseh_I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVuE1HHDDObv",
        "outputId": "e23d873e-a3e2-486e-d8fb-82abc6a05d4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Apr 15 08:56:13 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ke4ew9MGDPEJ",
        "outputId": "e9fb8004-2168-47b3-a763-8942448b5032"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "XrWUxFBpelcB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0M_2qhJ7Skl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import resize\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import os, dill\n",
        "import random, time\n",
        "import gym\n",
        "\n",
        "from collections import namedtuple, deque\n",
        "from copy import deepcopy, copy\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv8GkXFgSP8y",
        "outputId": "31aab97f-36d6-4706-e9cc-f56b10487e5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbwemBiS72PA",
        "outputId": "54c4365a-7a73-4041-aed4-5f656cc4b05e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrapper class for Pong\n",
        "\n",
        "[source](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/wrappers.py)"
      ],
      "metadata": {
        "id": "8_09aOJ2epIf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N67PnOzkkImh"
      },
      "outputs": [],
      "source": [
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        return obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh3uCEZt78K4"
      },
      "source": [
        "# Explore environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "T3pIm8NV780w",
        "outputId": "c03ac713-07ac-48a5-f71f-4d06bcbcd181"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = gym.make(\"PongDeterministic-v4\")\n",
        "env = FireResetEnv(env)\n",
        "state = env.reset()\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.imshow(state)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdGp1Tlw9pe3",
        "outputId": "42901929-1599-49f1-e1d2-fdaaeb2a072f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of possible actions: 6\n"
          ]
        }
      ],
      "source": [
        "print(f'Number of possible actions: {env.action_space.n}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMn3waeav1IK"
      },
      "outputs": [],
      "source": [
        "def play_one_episode():\n",
        "    done = False\n",
        "    s = env.reset()\n",
        "    reward = 0\n",
        "    while not done:\n",
        "        s, r, done, _ = env.step(env.action_space.sample())\n",
        "        reward += r\n",
        "\n",
        "    return reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOjezIsU-BhN"
      },
      "source": [
        "# Preprocessing:\n",
        "\n",
        "1. Convert to grayscale.\n",
        "2. Ressize to 75 x 80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE72hyAe-JlZ"
      },
      "outputs": [],
      "source": [
        "# # convert to gray scale\n",
        "# def convert_to_gray(img):\n",
        "#     return np.dot(img, [0.2989, 0.5870, 0.1140])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[source](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)"
      ],
      "metadata": {
        "id": "g3NZn7YjevCc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NkfOJ__kOU2"
      },
      "outputs": [],
      "source": [
        "def convert_to_gray_rescale(img):\n",
        "    \"\"\" prepro 210x160x3 uint8 frame into 6000 (75x80) 1D float vector \"\"\"\n",
        "    img = img[35:185] # crop - remove 35px from start & 25px from end of image in x, to reduce redundant parts of image (i.e. after ball passes paddle)\n",
        "    img = img[::2,::2,0] # downsample by factor of 2.\n",
        "    img[img == 144] = 0 # erase background (background type 1)\n",
        "    img[img == 109] = 0 # erase background (background type 2)\n",
        "    img[img != 0] = 1 # everything else (paddles, ball) just set to 1. this makes the image grayscale effectively\n",
        "    return img "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "t5YIepNL_GbB",
        "outputId": "1d51b467-b4f1-4e0d-87ff-7cb8f710af14"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "state_g = convert_to_gray_rescale(state)\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.imshow(state_g, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGKxFBjHx3zR"
      },
      "outputs": [],
      "source": [
        "def normalize(img):\n",
        "    return img / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrReW_0MAHr_"
      },
      "outputs": [],
      "source": [
        "# putting everything together\n",
        "def preprocess(img):\n",
        "    img_g = convert_to_gray_rescale(img)\n",
        "    #img_t = resize(img_g, (84, 84))\n",
        "    img_n = normalize(img_g)\n",
        "    return img_n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replay Buffer Class\n"
      ],
      "metadata": {
        "id": "_dNZnSfPezNg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU6rtZXwAq4J"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, env, capacity, n_frames):\n",
        "        self.env = env\n",
        "        self.capacity = capacity\n",
        "        self.n_frames = n_frames\n",
        "        self.buffer = namedtuple('Buffer', field_names=['state', 'action', 'reward', 'done','next_state'])\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "        \n",
        "    def push(self, state, action, reward, done, next_state):\n",
        "        # convert (state, action, next_state, reward, done)\n",
        "        self.memory.append(self.buffer(state, action, reward, done, next_state))\n",
        "        \n",
        "        \n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.memory), batch_size, replace=False)\n",
        "        batch = zip(*[self.memory[i] for i in indices])\n",
        "        return batch\n",
        "    \n",
        "    def populate(self, length):\n",
        "        # populate buffer\n",
        "        state_frames = deque(maxlen=self.n_frames)\n",
        "        next_state_frames = deque(maxlen=self.n_frames)\n",
        "        zeros = np.zeros((75,80))\n",
        "\n",
        "\n",
        "        while len(self.memory) < length:\n",
        "            # init frames\n",
        "            for i in range(self.n_frames):\n",
        "                state_frames.append(zeros)\n",
        "                next_state_frames.append(zeros)\n",
        "            \n",
        "            done = False\n",
        "            s0 = self.env.reset()\n",
        "            while not done:\n",
        "                action = self.env.action_space.sample()\n",
        "                s1, r, done, _ = self.env.step(action)\n",
        "\n",
        "                # build input states\n",
        "                s0_t = preprocess(s0) # (84, 84)\n",
        "                state_frames.append(s0_t) # stack of 4 (84,84)\n",
        "                s1_t = preprocess(s1) # (84, 84)\n",
        "                next_state_frames.append(s1_t) # stack of 4 (84,84)\n",
        "\n",
        "                # stack 4 frames in  tensor\n",
        "                input_state = np.stack(state_frames)\n",
        "                input_next_state = np.stack(next_state_frames)\n",
        "\n",
        "                # update buffer\n",
        "                self.push(input_state, action, r, done, input_next_state)\n",
        "                s0 = s1\n",
        "\n",
        "                if len(self.memory) >= length:\n",
        "                    break\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanilla Q-network class"
      ],
      "metadata": {
        "id": "-SP9a_iKe1go"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVqgc3tNQF4Z"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self,env, n_frames):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(n_frames, 32, kernel_size=8, stride=4), # input_shape: batch_size x 4 x 84 x 84 -> batch_size x 32 x 20 x 20\n",
        "            #nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            #nn.BatchNorm2d(64),\n",
        "            nn.ReLU(), \n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            #nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            )\n",
        "        \n",
        "        conv_out_shape = self._get_input_shape([4,75,80]) # replace (75,80) by your input shape\n",
        "\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_shape, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, env.action_space.n)\n",
        "        )\n",
        "              \n",
        "    def forward(self, x):\n",
        "        # conv layer output\n",
        "        features = self.conv(x)\n",
        "        features = features.view(x.size(0), -1)\n",
        "\n",
        "        q_values = self.fc(features)\n",
        "\n",
        "        return q_values\n",
        "        \n",
        "    def _get_input_shape(self, shape):\n",
        "        conv_out = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(conv_out.size()))\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent Class"
      ],
      "metadata": {
        "id": "1DsE_NV_e4C3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73c5BoOMTR_E"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self, env, buffer, epsilon, epsilon_mid, epsilon_final, decay_mid, decay_final):\n",
        "        self.env = env\n",
        "        self.buffer = buffer\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_start = epsilon\n",
        "        self.epsilon_mid = epsilon_mid\n",
        "        self.epsilon_final = epsilon_final\n",
        "        self.decay_mid = decay_mid\n",
        "        self.decay_final = decay_final\n",
        "        self.state_frames = deque(maxlen=buffer.n_frames)\n",
        "        self.next_state_frames = deque(maxlen=buffer.n_frames)\n",
        "        self.reset_frames()\n",
        "        \n",
        "    def take_action(self, state):\n",
        "        \n",
        "        # build state frames\n",
        "        state_frames = self.build_state_frames(state) # 4, 75, 80\n",
        "        # which action to take ?\n",
        "        action = self.get_action(state_frames)\n",
        "\n",
        "        # take the action\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "        # build next state frames\n",
        "        next_state_frames = self.build_next_state_frames(next_state)\n",
        "\n",
        "        # push new buffer\n",
        "        self.buffer.push(state_frames, action, reward, done, next_state_frames)\n",
        "        \n",
        "        return next_state, reward, done\n",
        "        \n",
        "    def get_action(self, state_frames):\n",
        "        if random.random() < self.epsilon:\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            action = self.get_greedy_action(state_frames)\n",
        "        return action\n",
        "            \n",
        "    def get_greedy_action(self, state_frames):\n",
        "        with torch.no_grad():\n",
        "            Q_network.eval()\n",
        "            state_frames = np.expand_dims(state_frames, 0)\n",
        "            state_frames = torch.FloatTensor(state_frames).to(device)\n",
        "            index_action = torch.argmax(Q_network(state_frames).cpu()).item()\n",
        "            Q_network.train()\n",
        "        return index_action\n",
        "    \n",
        "    \n",
        "    def learn(self, batch_size):\n",
        "        # sample a batch\n",
        "        batch = self.buffer.sample(batch_size)\n",
        "        states, actions, rewards, dones, next_states = [i for i in batch]\n",
        "\n",
        "        # transform batch to tensor then to device\n",
        "        states_t = torch.FloatTensor(np.array(states)).to(device)\n",
        "        actions_t = torch.LongTensor(np.array(actions)).to(device)\n",
        "        rewards_t = torch.FloatTensor(np.array(rewards)).to(device)\n",
        "        dones_t = torch.BoolTensor(np.array(dones)).to(device)\n",
        "        next_states_t = torch.FloatTensor(np.array(next_states)).to(device)\n",
        "\n",
        "        # update rule : q(s,a) = q(s,a) + (r + gamma * max_a'_q(s',a') - q(s,a))\n",
        "        # y = r + gamma * max_a'_q(s',a')\n",
        "        # get q_vals target\n",
        "        next_q_values = Q_network(next_states_t).detach().max(1)[0]# batch size, 1\n",
        "        next_q_values[dones_t] = 0 # zero the final states\n",
        "        y = rewards_t + gamma * next_q_values\n",
        "        y = y.unsqueeze(1)\n",
        "    \n",
        "        # q_values\n",
        "        q_values = Q_network(states_t).gather(1, actions_t.reshape(-1, 1))\n",
        "        \n",
        "        # compute loss\n",
        "        loss = criterion(q_values, y)\n",
        "        \n",
        "        # back prop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # # Trick: gradient clipping\n",
        "        for param in Q_network.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "\n",
        "        # gradient descent\n",
        "        optimizer.step()\n",
        "        \n",
        "        return loss.item()\n",
        "         \n",
        "    def reset_frames(self):\n",
        "        zeros = np.zeros((75,80))\n",
        "        for i in range(self.buffer.n_frames):\n",
        "            self.state_frames.append(zeros)\n",
        "            self.next_state_frames.append(zeros)\n",
        "\n",
        "    def build_state_frames(self, state):\n",
        "        state_t = preprocess(state)\n",
        "        self.state_frames.append(state_t)\n",
        "        return np.stack(self.state_frames)\n",
        "\n",
        "    def build_next_state_frames(self, next_state):\n",
        "        next_state_t = preprocess(next_state)\n",
        "        self.next_state_frames.append(next_state_t)\n",
        "        return np.stack(self.next_state_frames)\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        e = self.epsilon \n",
        "\n",
        "        if e > self.epsilon_mid:\n",
        "            self.epsilon = max(self.epsilon_mid, e - self.decay_mid)\n",
        "        else:\n",
        "            self.epsilon = max(self.epsilon_final, e - self.decay_final)\n",
        " \n",
        "    def reset_epsilon(self):\n",
        "        self.epsilon = self.epsilon_start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_WnTPusRe4F"
      },
      "outputs": [],
      "source": [
        "def copy_network(Q_network, Q_target):\n",
        "    Q_target.load_state_dict(Q_network.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standard Weight Initialization function"
      ],
      "metadata": {
        "id": "BF8dR_gEfCCw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBEf6oAZiv8p"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(m):\n",
        "  if isinstance(m, nn.Conv2d):\n",
        "      nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
        "      if m.bias is not None:\n",
        "          nn.init.constant_(m.bias.data, 0)\n",
        "#   elif isinstance(m, nn.BatchNorm2d):\n",
        "#       nn.init.constant_(m.weight.data, 1)\n",
        "#       nn.init.constant_(m.bias.data, 0)\n",
        "  elif isinstance(m, nn.Linear):\n",
        "      nn.init.kaiming_uniform_(m.weight.data)\n",
        "      nn.init.constant_(m.bias.data, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "ZxQsmszsfDM3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z79ssfZ1kFcE"
      },
      "outputs": [],
      "source": [
        "# Hyper params \n",
        "capacity = 40000\n",
        "n_frames = 4\n",
        "epsilon = 1\n",
        "epsilon_mid = 0.1\n",
        "epsilon_final = 0.02\n",
        "initial_buffer_size = 10000\n",
        "mid_exploration_frame = 1000000\n",
        "final_exploration_frame = 2400000\n",
        "\n",
        "decay_mid =  (epsilon - epsilon_mid) / mid_exploration_frame\n",
        "\n",
        "decay_final = (epsilon_mid - epsilon_final) / (final_exploration_frame - mid_exploration_frame)\n",
        "\n",
        "\n",
        "lr = 1e-4 #25e-5 # ok for pong\n",
        "batch_size = 32\n",
        "\n",
        "max_episodes = 5000\n",
        "rewards_target = 19\n",
        "window = 100\n",
        "network_update_frequency = 4\n",
        "#network_sync_frequency = 1000\n",
        "network_epsilon_update_frequency = 1\n",
        "gamma = 0.99\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate and populate buffer"
      ],
      "metadata": {
        "id": "pCuYMzATfF_M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW9vFtEykR6Q",
        "outputId": "ae872145-fd72-4fda-b530-8044038e7fee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It took : 0:00:08.377611 to populate buffer of size: 10000\n"
          ]
        }
      ],
      "source": [
        "buffer = ReplayBuffer(env, capacity, n_frames)\n",
        "t0 = datetime.now()\n",
        "buffer.populate(initial_buffer_size)\n",
        "dt = datetime.now() - t0\n",
        "\n",
        "print(\"It took :\", dt, \"to populate buffer of size:\", len(buffer))\n",
        "agent = Agent(env, buffer, epsilon, epsilon_mid, epsilon_final, decay_mid, decay_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enfzqHLor0jN"
      },
      "source": [
        "# Save / Load functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BQHFrTTrz1Z"
      },
      "outputs": [],
      "source": [
        "drive_path = \"/content/drive/MyDrive/PongDeterministic-v4-DQN\"\n",
        "\n",
        "if not os.path.exists(drive_path):\n",
        "    os.mkdir(drive_path)\n",
        "data_names = ['models', 'mean_rewards', 'best_score', 'losses']\n",
        "def save_data(episode, reward):\n",
        "\n",
        "    print(\"saving to drive\")\n",
        "\n",
        "    for data in data_names:\n",
        "        data_path = os.path.join(drive_path, data)\n",
        "        if not os.path.exists(data_path):\n",
        "            os.mkdir(data_path)\n",
        "\n",
        "        if data == 'models':\n",
        "\n",
        "            data_file_name = './pong_ep_{}_score_{}.pth'.format(episode, reward)\n",
        "            data_file_path = os.path.join(data_path, data_file_name)\n",
        "\n",
        "            torch.save(Q_network, data_file_path)\n",
        "            print('model saved')        \n",
        "\n",
        "        else:\n",
        "            print(\"Saving \" + data )\n",
        "            data_file_name = data + f\"_{episode}_{reward}.pkl\"\n",
        "            data_file_path = os.path.join(data_path, data_file_name)\n",
        "\n",
        "            with open(data_file_path, 'wb') as f:\n",
        "                if data == 'mean_rewards':\n",
        "                    dill.dump(mean_episode_rewards, f)\n",
        "                elif data == 'best_score':\n",
        "                    dill.dump(best_score, f)\n",
        "                elif data == 'losses':\n",
        "                    dill.dump(episodes_loss, f)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wle85688r6TZ"
      },
      "outputs": [],
      "source": [
        "# 10652 _ 1873\n",
        "def load_data(episode, reward):\n",
        "\n",
        "    print(\"loading from drive\")\n",
        "\n",
        "    for data in data_names:\n",
        "        data_path = os.path.join(drive_path, data)\n",
        "\n",
        "        if data == 'models':\n",
        "            data_file_name = f\"pong_ep_{episode}_score_{reward}.pth\"\n",
        "            data_file_path = os.path.join(data_path, data_file_name)\n",
        "\n",
        "            Q_network = torch.load(data_file_path, map_location=device)\n",
        "            Q_target = torch.load(data_file_path, map_location=device)\n",
        "            print(\"Models Loaded\")\n",
        "            print(Q_network)\n",
        "        \n",
        "        else:\n",
        "            print(\"Loading \" + data)\n",
        "            data_file_name = data + f\"_{episode}_{reward}.pkl\"\n",
        "            data_file_path = os.path.join(data_path, data_file_name)\n",
        "            with open(data_file_path, 'rb') as f:\n",
        "                if data == 'mean_rewards':\n",
        "                    mean_episode_rewards = dill.load(f)\n",
        "                    print(\"Rewards loaded, length of rewards:\", len(mean_episode_rewards))\n",
        "                elif data == 'best_score':\n",
        "                    best_score = dill.load(f)\n",
        "                    print(\"best score loaded:\", best_score)\n",
        "                elif data == 'losses':\n",
        "                    episodes_loss = dill.load(f)\n",
        "                    print(\"episodes_loss loaded, length of losses:\", len(episodes_loss))\n",
        "\n",
        "    return Q_network, Q_target, mean_episode_rewards, best_score, episodes_loss\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate model"
      ],
      "metadata": {
        "id": "s58HGgCSfJv0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnvkRiYKZE0L",
        "outputId": "2abe616f-8b6f-4eb5-e5b8-c6075159a587"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "QNetwork(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
              "    (3): ReLU()\n",
              "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (5): ReLU()\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=1920, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Q_network = QNetwork(env, n_frames)\n",
        "Q_network.apply(initialize_weights)\n",
        "Q_network.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WwGf5iU2wxa",
        "outputId": "cbf6942e-00d0-48d6-db67-eb382804879f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pong_ep_1000_score_-1697.pth  pong_ep_1587_score_-1223.pth\n",
            "pong_ep_1023_score_-1652.pth  pong_ep_1600_score_-1135.pth\n",
            "pong_ep_1041_score_-1570.pth  pong_ep_629_score_-1898.pth\n",
            "pong_ep_1053_score_-1477.pth  pong_ep_736_score_-1818.pth\n",
            "pong_ep_1076_score_-1387.pth  pong_ep_964_score_-1737.pth\n",
            "pong_ep_1120_score_-1305.pth\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/PongDeterministic-v4-DDQN/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3cU5lRO2u3q",
        "outputId": "66daa8f6-1021-4337-b957-94c1cdd2e07a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading from drive\n",
            "Models Loaded\n",
            "QNetwork(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=1920, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
            "  )\n",
            ")\n",
            "Loading mean_rewards\n",
            "Rewards loaded, length of rewards: 1601\n",
            "Loading best_score\n",
            "best score loaded: -11.35\n",
            "Loading losses\n",
            "episodes_loss loaded, length of losses: 1601\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "QNetwork(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
              "    (3): ReLU()\n",
              "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (5): ReLU()\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=1920, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# In case loading model\n",
        "# Q_network, _ , mean_episode_rewards, best_score, episodes_loss = load_data(1600,-1135)\n",
        "# Q_network.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "cseIlmttfNoE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDhuZX17eCWA"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(Q_network.parameters(), lr=lr)\n",
        "#optimizer = torch.optim.RMSprop(Q_network.parameters(), lr=lr)\n",
        "criterion = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SL7t9plReSDK"
      },
      "outputs": [],
      "source": [
        "# 10652 _ 1873\n",
        "#Q_network, Q_target, mean_episode_rewards, best_score, episodes_loss = load_data(10652, 1873)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujsong_IZGP1",
        "outputId": "3cdd51a3-6804-4ba2-eaa8-fbdd352b4698"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0 Mean Rewards -19.00 \n",
            "Total steps so far: 979\n",
            "Current epsilon: 1\n",
            "Episode 10 Mean Rewards -20.45 \n",
            "Episode 20 Mean Rewards -20.43 \n",
            "Episode 30 Mean Rewards -20.45 \n",
            "Episode 40 Mean Rewards -20.41 \n",
            "Episode 50 Mean Rewards -20.49 \n",
            "Episode 60 Mean Rewards -20.51 \n",
            "Episode 70 Mean Rewards -20.48 \n",
            "Episode 80 Mean Rewards -20.51 \n",
            "Episode 90 Mean Rewards -20.53 \n",
            "Episode 100 Mean Rewards -20.53 \n",
            "Total steps so far: 90897\n",
            "Current epsilon: 0.9631918000012119\n",
            "Episode 110 Mean Rewards -20.50 \n",
            "Episode 120 Mean Rewards -20.50 \n",
            "Episode 130 Mean Rewards -20.49 \n",
            "Episode 140 Mean Rewards -20.53 \n",
            "Episode 150 Mean Rewards -20.51 \n",
            "Episode 160 Mean Rewards -20.43 \n",
            "Episode 170 Mean Rewards -20.43 \n",
            "Episode 180 Mean Rewards -20.40 \n",
            "Episode 190 Mean Rewards -20.41 \n",
            "Episode 200 Mean Rewards -20.42 \n",
            "Total steps so far: 183597\n",
            "Current epsilon: 0.8797618000039586\n",
            "Episode 210 Mean Rewards -20.42 \n",
            "Episode 220 Mean Rewards -20.43 \n",
            "Episode 230 Mean Rewards -20.38 \n",
            "Episode 240 Mean Rewards -20.33 \n",
            "Episode 250 Mean Rewards -20.37 \n",
            "Episode 260 Mean Rewards -20.46 \n",
            "Episode 270 Mean Rewards -20.45 \n",
            "Episode 280 Mean Rewards -20.46 \n",
            "Episode 290 Mean Rewards -20.40 \n",
            "Episode 300 Mean Rewards -20.37 \n",
            "Total steps so far: 275239\n",
            "Current epsilon: 0.7972840000066741\n",
            "Episode 310 Mean Rewards -20.39 \n",
            "Episode 320 Mean Rewards -20.41 \n",
            "Episode 330 Mean Rewards -20.48 \n",
            "Episode 340 Mean Rewards -20.52 \n",
            "Episode 350 Mean Rewards -20.45 \n",
            "Episode 360 Mean Rewards -20.43 \n",
            "Episode 370 Mean Rewards -20.47 \n",
            "Episode 380 Mean Rewards -20.44 \n",
            "Episode 390 Mean Rewards -20.45 \n",
            "Episode 400 Mean Rewards -20.51 \n",
            "Total steps so far: 366178\n",
            "Current epsilon: 0.7154389000093687\n",
            "Episode 410 Mean Rewards -20.54 \n",
            "Episode 420 Mean Rewards -20.54 \n",
            "Episode 430 Mean Rewards -20.50 \n",
            "Episode 440 Mean Rewards -20.47 \n",
            "Episode 450 Mean Rewards -20.48 \n",
            "Episode 460 Mean Rewards -20.47 \n",
            "Episode 470 Mean Rewards -20.45 \n",
            "Episode 480 Mean Rewards -20.49 \n",
            "Episode 490 Mean Rewards -20.51 \n",
            "Episode 500 Mean Rewards -20.46 \n",
            "Total steps so far: 456782\n",
            "Current epsilon: 0.6338953000120534\n",
            "Episode 510 Mean Rewards -20.39 \n",
            "Episode 520 Mean Rewards -20.40 \n",
            "Episode 530 Mean Rewards -20.44 \n",
            "Episode 540 Mean Rewards -20.43 \n",
            "Episode 550 Mean Rewards -20.39 \n",
            "Episode 560 Mean Rewards -20.36 \n",
            "Episode 570 Mean Rewards -20.34 \n",
            "Episode 580 Mean Rewards -20.30 \n",
            "Episode 590 Mean Rewards -20.26 \n",
            "Episode 600 Mean Rewards -20.27 \n",
            "Total steps so far: 549518\n",
            "Current epsilon: 0.5504329000148013\n",
            "Episode 610 Mean Rewards -20.22 \n",
            "Episode 620 Mean Rewards -20.16 \n",
            "Episode 630 Mean Rewards -20.13 \n",
            "Episode 640 Mean Rewards -20.17 \n",
            "Episode 650 Mean Rewards -20.20 \n",
            "Episode 660 Mean Rewards -20.18 \n",
            "Episode 670 Mean Rewards -20.19 \n",
            "Episode 680 Mean Rewards -20.20 \n",
            "Episode 690 Mean Rewards -20.18 \n",
            "Episode 700 Mean Rewards -20.21 \n",
            "Total steps so far: 641450\n",
            "Current epsilon: 0.4676941000155327\n",
            "Episode 710 Mean Rewards -20.25 \n",
            "Episode 720 Mean Rewards -20.25 \n",
            "Episode 730 Mean Rewards -20.24 \n",
            "Episode 740 Mean Rewards -20.22 \n",
            "Episode 750 Mean Rewards -20.22 \n",
            "Episode 760 Mean Rewards -20.27 \n",
            "Episode 770 Mean Rewards -20.29 \n",
            "Episode 780 Mean Rewards -20.27 \n",
            "Episode 790 Mean Rewards -20.35 \n",
            "Episode 800 Mean Rewards -20.29 \n",
            "Total steps so far: 735875\n",
            "Current epsilon: 0.38271160001308896\n",
            "Episode 810 Mean Rewards -20.38 \n",
            "Episode 820 Mean Rewards -20.42 \n",
            "Episode 830 Mean Rewards -20.45 \n",
            "Episode 840 Mean Rewards -20.43 \n",
            "Episode 850 Mean Rewards -20.41 \n",
            "Episode 860 Mean Rewards -20.36 \n",
            "Episode 870 Mean Rewards -20.35 \n",
            "Episode 880 Mean Rewards -20.34 \n",
            "Episode 890 Mean Rewards -20.21 \n",
            "Episode 900 Mean Rewards -20.20 \n",
            "Total steps so far: 835195\n",
            "Current epsilon: 0.29332360001051855\n",
            "Episode 910 Mean Rewards -20.07 \n",
            "Episode 920 Mean Rewards -19.99 \n",
            "Episode 930 Mean Rewards -19.98 \n",
            "Episode 940 Mean Rewards -19.93 \n",
            "Episode 950 Mean Rewards -19.95 \n",
            "Episode 960 Mean Rewards -19.89 \n",
            "Episode 970 Mean Rewards -19.86 \n",
            "Episode 980 Mean Rewards -19.79 \n",
            "Episode 990 Mean Rewards -19.67 \n",
            "Episode 1000 Mean Rewards -19.54 \n",
            "Total steps so far: 959799\n",
            "Current epsilon: 0.18118000000941617\n",
            "Check point\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Episode 1010 Mean Rewards -19.58 \n",
            "Episode 1020 Mean Rewards -19.36 \n",
            "Episode 1030 Mean Rewards -19.34 \n",
            "Episode 1040 Mean Rewards -19.32 \n",
            "Episode 1050 Mean Rewards -19.06 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1031651. New best score: -18.99\n",
            "Episode 1060 Mean Rewards -19.01 \n",
            "Episode 1070 Mean Rewards -18.80 \n",
            "Episode 1080 Mean Rewards -18.53 \n",
            "Episode 1090 Mean Rewards -18.58 \n",
            "Episode 1100 Mean Rewards -18.50 \n",
            "Total steps so far: 1119608\n",
            "Current epsilon: 0.09602240000016163\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1139998. New best score: -18.11\n",
            "Episode 1110 Mean Rewards -18.03 \n",
            "Episode 1120 Mean Rewards -17.96 \n",
            "Episode 1130 Mean Rewards -17.45 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1191533. New best score: -17.27\n",
            "Episode 1140 Mean Rewards -17.15 \n",
            "Episode 1150 Mean Rewards -16.90 \n",
            "Episode 1160 Mean Rewards -17.04 \n",
            "Episode 1170 Mean Rewards -17.19 \n",
            "Episode 1180 Mean Rewards -16.94 \n",
            "Episode 1190 Mean Rewards -16.77 \n",
            "Episode 1200 Mean Rewards -16.79 \n",
            "Total steps so far: 1315246\n",
            "Current epsilon: 0.0848430857149016\n",
            "Episode 1210 Mean Rewards -17.01 \n",
            "Episode 1220 Mean Rewards -16.49 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1390478. New best score: -16.44\n",
            "Episode 1230 Mean Rewards -16.44 \n",
            "Episode 1240 Mean Rewards -15.91 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1440930. New best score: -15.63\n",
            "Episode 1250 Mean Rewards -15.68 \n",
            "Episode 1260 Mean Rewards -15.00 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1481381. New best score: -14.8\n",
            "Episode 1270 Mean Rewards -14.59 \n",
            "Episode 1280 Mean Rewards -14.78 \n",
            "Episode 1290 Mean Rewards -14.80 \n",
            "Episode 1300 Mean Rewards -14.71 \n",
            "Total steps so far: 1556055\n",
            "Current epsilon: 0.07108257142974644\n",
            "Episode 1310 Mean Rewards -14.32 \n",
            "Episode 1320 Mean Rewards -14.75 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1633989. New best score: -13.94\n",
            "Episode 1330 Mean Rewards -13.99 \n",
            "Episode 1340 Mean Rewards -14.00 \n",
            "Episode 1350 Mean Rewards -13.60 \n",
            "Episode 1360 Mean Rewards -13.26 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1723921. New best score: -13.09\n",
            "Episode 1370 Mean Rewards -12.69 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1770982. New best score: -12.24\n",
            "Episode 1380 Mean Rewards -12.24 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 1798081. New best score: -11.43\n",
            "Episode 1390 Mean Rewards -11.43 \n",
            "Episode 1400 Mean Rewards -11.25 \n",
            "Total steps so far: 1815459\n",
            "Current epsilon: 0.05625948571606304\n",
            "Episode 1410 Mean Rewards -11.80 \n",
            "Episode 1420 Mean Rewards -12.25 \n",
            "Episode 1430 Mean Rewards -13.61 \n",
            "Episode 1440 Mean Rewards -14.54 \n",
            "Episode 1450 Mean Rewards -15.73 \n",
            "Episode 1460 Mean Rewards -16.82 \n",
            "Episode 1470 Mean Rewards -17.93 \n",
            "Episode 1480 Mean Rewards -18.88 \n",
            "Episode 1490 Mean Rewards -20.05 \n",
            "Episode 1500 Mean Rewards -20.57 \n",
            "Total steps so far: 1902802\n",
            "Current epsilon: 0.05126845714483727\n",
            "Episode 1510 Mean Rewards -20.75 \n",
            "Episode 1520 Mean Rewards -20.78 \n",
            "Episode 1530 Mean Rewards -20.76 \n",
            "Episode 1540 Mean Rewards -20.77 \n",
            "Episode 1550 Mean Rewards -20.71 \n",
            "Episode 1560 Mean Rewards -20.66 \n",
            "Episode 1570 Mean Rewards -20.64 \n",
            "Episode 1580 Mean Rewards -20.63 \n",
            "Episode 1590 Mean Rewards -20.65 \n",
            "Episode 1600 Mean Rewards -20.67 \n",
            "Total steps so far: 1988447\n",
            "Current epsilon: 0.04637445714503613\n",
            "Episode 1610 Mean Rewards -20.64 \n",
            "Episode 1620 Mean Rewards -20.62 \n",
            "Episode 1630 Mean Rewards -20.62 \n",
            "Episode 1640 Mean Rewards -20.62 \n",
            "Episode 1650 Mean Rewards -20.70 \n",
            "Episode 1660 Mean Rewards -20.72 \n",
            "Episode 1670 Mean Rewards -20.73 \n",
            "Episode 1680 Mean Rewards -20.66 \n",
            "Episode 1690 Mean Rewards -20.62 \n",
            "Episode 1700 Mean Rewards -20.59 \n",
            "Total steps so far: 2081486\n",
            "Current epsilon: 0.04105794285953787\n",
            "Episode 1710 Mean Rewards -20.56 \n",
            "Episode 1720 Mean Rewards -20.59 \n",
            "Episode 1730 Mean Rewards -20.60 \n",
            "Episode 1740 Mean Rewards -20.57 \n",
            "Episode 1750 Mean Rewards -20.51 \n",
            "Episode 1760 Mean Rewards -20.50 \n",
            "Episode 1770 Mean Rewards -20.49 \n",
            "Episode 1780 Mean Rewards -20.54 \n",
            "Episode 1790 Mean Rewards -20.55 \n",
            "Episode 1800 Mean Rewards -20.56 \n",
            "Total steps so far: 2183401\n",
            "Current epsilon: 0.03523422857406022\n",
            "Episode 1810 Mean Rewards -20.58 \n",
            "Episode 1820 Mean Rewards -20.55 \n",
            "Episode 1830 Mean Rewards -20.46 \n",
            "Episode 1840 Mean Rewards -20.42 \n",
            "Episode 1850 Mean Rewards -20.36 \n",
            "Episode 1860 Mean Rewards -20.29 \n",
            "Episode 1870 Mean Rewards -20.09 \n",
            "Episode 1880 Mean Rewards -19.77 \n",
            "Episode 1890 Mean Rewards -19.39 \n",
            "Episode 1900 Mean Rewards -18.75 \n",
            "Total steps so far: 2346308\n",
            "Current epsilon: 0.025925257145543754\n",
            "Episode 1910 Mean Rewards -17.97 \n",
            "Episode 1920 Mean Rewards -16.36 \n",
            "Episode 1930 Mean Rewards -14.92 \n",
            "Episode 1940 Mean Rewards -12.99 \n",
            "Episode 1950 Mean Rewards -11.96 \n",
            "Episode 1960 Mean Rewards -11.36 \n",
            "Episode 1970 Mean Rewards -10.95 \n",
            "Episode 1980 Mean Rewards -10.76 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 2565722. New best score: -10.61\n",
            "Episode 1990 Mean Rewards -10.67 \n",
            "Episode 2000 Mean Rewards -10.92 \n",
            "Total steps so far: 2604273\n",
            "Check point\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Episode 2010 Mean Rewards -10.97 \n",
            "Episode 2020 Mean Rewards -12.03 \n",
            "Episode 2030 Mean Rewards -12.70 \n",
            "Episode 2040 Mean Rewards -13.67 \n",
            "Episode 2050 Mean Rewards -13.94 \n",
            "Episode 2060 Mean Rewards -13.82 \n",
            "Episode 2070 Mean Rewards -14.05 \n",
            "Episode 2080 Mean Rewards -13.20 \n",
            "Episode 2090 Mean Rewards -12.03 \n",
            "Episode 2100 Mean Rewards -10.88 \n",
            "Total steps so far: 2884270\n",
            "Episode 2110 Mean Rewards -10.60 \n",
            "Episode 2120 Mean Rewards -10.37 \n",
            "Episode 2130 Mean Rewards -10.38 \n",
            "Episode 2140 Mean Rewards -10.12 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3025787. New best score: -9.79\n",
            "Episode 2150 Mean Rewards -9.09 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3047860. New best score: -8.9\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3063086. New best score: -7.97\n",
            "Episode 2160 Mean Rewards -7.36 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3078309. New best score: -7.09\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3088496. New best score: -6.18\n",
            "Episode 2170 Mean Rewards -5.66 \n",
            "Episode 2180 Mean Rewards -5.75 \n",
            "Episode 2190 Mean Rewards -6.34 \n",
            "Episode 2200 Mean Rewards -7.02 \n",
            "Total steps so far: 3199214\n",
            "Episode 2210 Mean Rewards -6.30 \n",
            "Episode 2220 Mean Rewards -5.55 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3267348. New best score: -5.31\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3295755. New best score: -4.49\n",
            "Episode 2230 Mean Rewards -4.48 \n",
            "Episode 2240 Mean Rewards -4.01 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3367826. New best score: -3.69\n",
            "Episode 2250 Mean Rewards -3.34 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3412113. New best score: -2.88\n",
            "Episode 2260 Mean Rewards -2.62 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3437910. New best score: -1.96\n",
            "Episode 2270 Mean Rewards -1.71 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3464101. New best score: -1.07\n",
            "Episode 2280 Mean Rewards -0.83 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 3487003. New best score: -0.22\n",
            "Episode 2290 Mean Rewards -0.46 \n",
            "Episode 2300 Mean Rewards -0.03 \n",
            "Total steps so far: 3533236\n",
            "Episode 2310 Mean Rewards -0.53 \n",
            "Episode 2320 Mean Rewards -1.33 \n",
            "Episode 2330 Mean Rewards -2.30 \n",
            "Episode 2340 Mean Rewards -3.36 \n",
            "Episode 2350 Mean Rewards -4.84 \n",
            "Episode 2360 Mean Rewards -6.90 \n",
            "Episode 2370 Mean Rewards -8.82 \n",
            "Episode 2380 Mean Rewards -9.00 \n",
            "Episode 2390 Mean Rewards -8.46 \n",
            "Episode 2400 Mean Rewards -8.19 \n",
            "Total steps so far: 3829473\n",
            "Episode 2410 Mean Rewards -6.97 \n",
            "Episode 2420 Mean Rewards -6.10 \n",
            "Episode 2430 Mean Rewards -5.43 \n",
            "Episode 2440 Mean Rewards -4.33 \n",
            "Episode 2450 Mean Rewards -3.78 \n",
            "Episode 2460 Mean Rewards -2.06 \n",
            "Episode 2470 Mean Rewards -1.73 \n",
            "Episode 2480 Mean Rewards -3.21 \n",
            "Episode 2490 Mean Rewards -4.51 \n",
            "Episode 2500 Mean Rewards -5.37 \n",
            "Total steps so far: 4087046\n",
            "Episode 2510 Mean Rewards -6.73 \n",
            "Episode 2520 Mean Rewards -7.14 \n",
            "Episode 2530 Mean Rewards -7.35 \n",
            "Episode 2540 Mean Rewards -6.87 \n",
            "Episode 2550 Mean Rewards -5.74 \n",
            "Episode 2560 Mean Rewards -5.46 \n",
            "Episode 2570 Mean Rewards -3.91 \n",
            "Episode 2580 Mean Rewards -2.61 \n",
            "Episode 2590 Mean Rewards -1.43 \n",
            "Episode 2600 Mean Rewards 0.14 \n",
            "Total steps so far: 4416059\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 4434975. New best score: 0.68\n",
            "Episode 2610 Mean Rewards 1.48 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 4462192. New best score: 1.71\n",
            "Episode 2620 Mean Rewards 2.35 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 4506076. New best score: 2.58\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 4534194. New best score: 3.52\n",
            "Episode 2630 Mean Rewards 3.52 \n",
            "Episode 2640 Mean Rewards 3.41 \n",
            "Episode 2650 Mean Rewards 3.29 \n",
            "Episode 2660 Mean Rewards 2.56 \n",
            "Episode 2670 Mean Rewards 3.08 \n",
            "Episode 2680 Mean Rewards 4.09 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 4687324. New best score: 4.52\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 4699645. New best score: 5.53\n",
            "Episode 2690 Mean Rewards 5.86 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 4710598. New best score: 6.4\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 4730817. New best score: 7.22\n",
            "Episode 2700 Mean Rewards 7.22 \n",
            "Total steps so far: 4730817\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 4755348. New best score: 8.05\n",
            "Episode 2710 Mean Rewards 8.21 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 4771649. New best score: 8.86\n",
            "Episode 2720 Mean Rewards 9.61 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 4792210. New best score: 9.77\n",
            "Episode 2730 Mean Rewards 10.03 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 4833723. New best score: 10.59\n",
            "Episode 2740 Mean Rewards 9.88 \n",
            "Episode 2750 Mean Rewards 9.12 \n",
            "Episode 2760 Mean Rewards 8.83 \n",
            "Episode 2770 Mean Rewards 8.60 \n",
            "Episode 2780 Mean Rewards 8.11 \n",
            "Episode 2790 Mean Rewards 6.87 \n",
            "Episode 2800 Mean Rewards 6.13 \n",
            "Total steps so far: 5031907\n",
            "Episode 2810 Mean Rewards 4.71 \n",
            "Episode 2820 Mean Rewards 3.13 \n",
            "Episode 2830 Mean Rewards 2.15 \n",
            "Episode 2840 Mean Rewards 2.26 \n",
            "Episode 2850 Mean Rewards 3.03 \n",
            "Episode 2860 Mean Rewards 4.18 \n",
            "Episode 2870 Mean Rewards 4.89 \n",
            "Episode 2880 Mean Rewards 4.68 \n",
            "Episode 2890 Mean Rewards 4.74 \n",
            "Episode 2900 Mean Rewards 4.96 \n",
            "Total steps so far: 5329442\n",
            "Episode 2910 Mean Rewards 4.23 \n",
            "Episode 2920 Mean Rewards 5.58 \n",
            "Episode 2930 Mean Rewards 6.15 \n",
            "Episode 2940 Mean Rewards 7.20 \n",
            "Episode 2950 Mean Rewards 7.82 \n",
            "Episode 2960 Mean Rewards 7.59 \n",
            "Episode 2970 Mean Rewards 6.88 \n",
            "Episode 2980 Mean Rewards 8.19 \n",
            "Episode 2990 Mean Rewards 9.12 \n",
            "Episode 3000 Mean Rewards 9.23 \n",
            "Total steps so far: 5636049\n",
            "Check point\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 5652631. New best score: 11.45\n",
            "Episode 3010 Mean Rewards 11.85 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 5679541. New best score: 12.27\n",
            "Episode 3020 Mean Rewards 12.18 \n",
            "Episode 3030 Mean Rewards 12.50 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 5735129. New best score: 13.14\n",
            "Episode 3040 Mean Rewards 13.09 \n",
            "Episode 3050 Mean Rewards 12.43 \n",
            "Episode 3060 Mean Rewards 11.89 \n",
            "Episode 3070 Mean Rewards 10.59 \n",
            "Episode 3080 Mean Rewards 9.99 \n",
            "Episode 3090 Mean Rewards 9.62 \n",
            "Episode 3100 Mean Rewards 9.90 \n",
            "Total steps so far: 5908847\n",
            "Episode 3110 Mean Rewards 8.70 \n",
            "Episode 3120 Mean Rewards 7.53 \n",
            "Episode 3130 Mean Rewards 7.10 \n",
            "Episode 3140 Mean Rewards 7.05 \n",
            "Episode 3150 Mean Rewards 8.08 \n",
            "Episode 3160 Mean Rewards 7.79 \n",
            "Episode 3170 Mean Rewards 7.78 \n",
            "Episode 3180 Mean Rewards 7.59 \n",
            "Episode 3190 Mean Rewards 6.55 \n",
            "Episode 3200 Mean Rewards 5.35 \n",
            "Total steps so far: 6232579\n",
            "Episode 3210 Mean Rewards 4.83 \n",
            "Episode 3220 Mean Rewards 4.15 \n",
            "Episode 3230 Mean Rewards 3.30 \n",
            "Episode 3240 Mean Rewards 2.02 \n",
            "Episode 3250 Mean Rewards 1.04 \n",
            "Episode 3260 Mean Rewards 1.75 \n",
            "Episode 3270 Mean Rewards 2.17 \n",
            "Episode 3280 Mean Rewards 3.32 \n",
            "Episode 3290 Mean Rewards 5.01 \n",
            "Episode 3300 Mean Rewards 5.93 \n",
            "Total steps so far: 6530213\n",
            "Episode 3310 Mean Rewards 7.06 \n",
            "Episode 3320 Mean Rewards 8.60 \n",
            "Episode 3330 Mean Rewards 8.85 \n",
            "Episode 3340 Mean Rewards 8.56 \n",
            "Episode 3350 Mean Rewards 8.32 \n",
            "Episode 3360 Mean Rewards 7.55 \n",
            "Episode 3370 Mean Rewards 7.48 \n",
            "Episode 3380 Mean Rewards 6.46 \n",
            "Episode 3390 Mean Rewards 6.01 \n",
            "Episode 3400 Mean Rewards 6.06 \n",
            "Total steps so far: 6877456\n",
            "Episode 3410 Mean Rewards 6.05 \n",
            "Episode 3420 Mean Rewards 6.31 \n",
            "Episode 3430 Mean Rewards 7.49 \n",
            "Episode 3440 Mean Rewards 8.46 \n",
            "Episode 3450 Mean Rewards 9.83 \n",
            "Episode 3460 Mean Rewards 11.29 \n",
            "Episode 3470 Mean Rewards 12.60 \n",
            "Episode 3480 Mean Rewards 13.07 \n",
            "Episode 3490 Mean Rewards 12.91 \n",
            "Episode 3500 Mean Rewards 13.00 \n",
            "Total steps so far: 7185727\n",
            "Episode 3510 Mean Rewards 13.12 \n",
            "Episode 3520 Mean Rewards 12.34 \n",
            "Episode 3530 Mean Rewards 12.37 \n",
            "Episode 3540 Mean Rewards 12.52 \n",
            "Episode 3550 Mean Rewards 12.25 \n",
            "Episode 3560 Mean Rewards 11.76 \n",
            "Episode 3570 Mean Rewards 11.60 \n",
            "Episode 3580 Mean Rewards 11.85 \n",
            "Episode 3590 Mean Rewards 11.95 \n",
            "Episode 3600 Mean Rewards 12.17 \n",
            "Total steps so far: 7489526\n",
            "Episode 3610 Mean Rewards 12.53 \n",
            "Episode 3620 Mean Rewards 13.35 \n",
            "Episode 3630 Mean Rewards 13.27 \n",
            "Episode 3640 Mean Rewards 13.28 \n",
            "Episode 3650 Mean Rewards 13.54 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 7640833. New best score: 13.95\n",
            "Episode 3660 Mean Rewards 13.89 \n",
            "Episode 3670 Mean Rewards 13.88 \n",
            "Episode 3680 Mean Rewards 14.06 \n",
            "Episode 3690 Mean Rewards 14.59 \n",
            "Episode 3700 Mean Rewards 14.36 \n",
            "Total steps so far: 7756548\n",
            "Episode 3710 Mean Rewards 14.30 \n",
            "Episode 3720 Mean Rewards 14.50 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 7831757. New best score: 14.86\n",
            "Episode 3730 Mean Rewards 14.84 \n",
            "Episode 3740 Mean Rewards 15.08 \n",
            "Episode 3750 Mean Rewards 15.19 \n",
            "Episode 3760 Mean Rewards 14.76 \n",
            "Episode 3770 Mean Rewards 14.61 \n",
            "Episode 3780 Mean Rewards 14.45 \n",
            "Episode 3790 Mean Rewards 14.57 \n",
            "Episode 3800 Mean Rewards 14.82 \n",
            "Total steps so far: 8030768\n",
            "Episode 3810 Mean Rewards 14.52 \n",
            "Episode 3820 Mean Rewards 14.31 \n",
            "Episode 3830 Mean Rewards 14.58 \n",
            "Episode 3840 Mean Rewards 14.92 \n",
            "Episode 3850 Mean Rewards 14.89 \n",
            "Episode 3860 Mean Rewards 15.40 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 8206851. New best score: 15.75\n",
            "Episode 3870 Mean Rewards 15.76 \n",
            "Episode 3880 Mean Rewards 16.01 \n",
            "Episode 3890 Mean Rewards 15.86 \n",
            "Episode 3900 Mean Rewards 16.15 \n",
            "Total steps so far: 8294275\n",
            "Episode 3910 Mean Rewards 16.50 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 8324030. New best score: 16.57\n",
            "Episode 3920 Mean Rewards 16.75 \n",
            "Episode 3930 Mean Rewards 16.60 \n",
            "Episode 3940 Mean Rewards 16.12 \n",
            "Episode 3950 Mean Rewards 15.54 \n",
            "Episode 3960 Mean Rewards 15.68 \n",
            "Episode 3970 Mean Rewards 15.90 \n",
            "Episode 3980 Mean Rewards 15.85 \n",
            "Episode 3990 Mean Rewards 15.96 \n",
            "Episode 4000 Mean Rewards 15.54 \n",
            "Total steps so far: 8550182\n",
            "Check point\n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Episode 4010 Mean Rewards 15.72 \n",
            "Episode 4020 Mean Rewards 15.90 \n",
            "Episode 4030 Mean Rewards 15.86 \n",
            "Episode 4040 Mean Rewards 16.27 \n",
            "Episode 4050 Mean Rewards 16.79 \n",
            "Episode 4060 Mean Rewards 16.76 \n",
            "Episode 4070 Mean Rewards 16.45 \n",
            "Episode 4080 Mean Rewards 16.51 \n",
            "Episode 4090 Mean Rewards 16.57 \n",
            "Episode 4100 Mean Rewards 17.00 \n",
            "Total steps so far: 8803970\n",
            "Episode 4110 Mean Rewards 16.95 \n",
            "Episode 4120 Mean Rewards 16.93 \n",
            "Episode 4130 Mean Rewards 17.25 \n",
            "Episode 4140 Mean Rewards 17.34 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 8918638. New best score: 17.38\n",
            "Episode 4150 Mean Rewards 17.38 \n",
            "Episode 4160 Mean Rewards 17.58 \n",
            "Episode 4170 Mean Rewards 17.38 \n",
            "Episode 4180 Mean Rewards 17.63 \n",
            "Episode 4190 Mean Rewards 17.71 \n",
            "Episode 4200 Mean Rewards 17.43 \n",
            "Total steps so far: 9035892\n",
            "Episode 4210 Mean Rewards 17.37 \n",
            "Episode 4220 Mean Rewards 17.32 \n",
            "Episode 4230 Mean Rewards 17.39 \n",
            "Episode 4240 Mean Rewards 17.41 \n",
            "Episode 4250 Mean Rewards 17.59 \n",
            "Episode 4260 Mean Rewards 17.49 \n",
            "Episode 4270 Mean Rewards 17.38 \n",
            "Episode 4280 Mean Rewards 17.13 \n",
            "Episode 4290 Mean Rewards 16.39 \n",
            "Episode 4300 Mean Rewards 16.08 \n",
            "Total steps so far: 9271939\n",
            "Episode 4310 Mean Rewards 15.81 \n",
            "Episode 4320 Mean Rewards 15.36 \n",
            "Episode 4330 Mean Rewards 15.26 \n",
            "Episode 4340 Mean Rewards 15.20 \n",
            "Episode 4350 Mean Rewards 14.90 \n",
            "Episode 4360 Mean Rewards 14.90 \n",
            "Episode 4370 Mean Rewards 15.53 \n",
            "Episode 4380 Mean Rewards 15.79 \n",
            "Episode 4390 Mean Rewards 16.59 \n",
            "Episode 4400 Mean Rewards 17.17 \n",
            "Total steps so far: 9504900\n",
            "Episode 4410 Mean Rewards 17.47 \n",
            "Episode 4420 Mean Rewards 17.91 \n",
            "Episode 4430 Mean Rewards 18.02 \n",
            "Episode 4440 Mean Rewards 17.97 \n",
            "Episode 4450 Mean Rewards 18.06 \n",
            "Episode 4460 Mean Rewards 17.92 \n",
            "Episode 4470 Mean Rewards 17.94 \n",
            "Episode 4480 Mean Rewards 17.80 \n",
            "Episode 4490 Mean Rewards 17.70 \n",
            "Episode 4500 Mean Rewards 17.70 \n",
            "Total steps so far: 9738367\n",
            "Episode 4510 Mean Rewards 17.66 \n",
            "Episode 4520 Mean Rewards 17.71 \n",
            "Episode 4530 Mean Rewards 17.77 \n",
            "Episode 4540 Mean Rewards 17.93 \n",
            "Episode 4550 Mean Rewards 18.06 \n",
            "saving to drive\n",
            "model saved\n",
            "Saving mean_rewards\n",
            "Saving best_score\n",
            "Saving losses\n",
            "Model Saved. Total Steps: 9860142. New best score: 18.2\n",
            "Episode 4560 Mean Rewards 18.35 \n",
            "Episode 4570 Mean Rewards 18.43 \n",
            "Episode 4580 Mean Rewards 18.42 \n",
            "Episode 4590 Mean Rewards 18.51 \n",
            "Episode 4600 Mean Rewards 18.54 \n",
            "Total steps so far: 9967220\n",
            "Episode 4610 Mean Rewards 18.70 \n",
            "Episode 4620 Mean Rewards 18.38 \n",
            "Episode 4630 Mean Rewards 18.43 \n",
            "Episode 4640 Mean Rewards 18.48 \n",
            "Episode 4650 Mean Rewards 18.54 \n",
            "Episode 4660 Mean Rewards 18.56 \n",
            "Episode 4670 Mean Rewards 18.62 \n",
            "Episode 4680 Mean Rewards 18.73 \n",
            "Episode 4690 Mean Rewards 18.76 \n",
            "Episode 4700 Mean Rewards 18.77 \n",
            "Total steps so far: 10187090\n",
            "Episode 4710 Mean Rewards 18.63 \n",
            "Episode 4720 Mean Rewards 18.99 \n",
            "Episode 4730 Mean Rewards 18.94 \n",
            "Episode 4740 Mean Rewards 18.83 \n",
            "Episode 4750 Mean Rewards 18.72 \n",
            "Episode 4760 Mean Rewards 18.72 \n",
            "Episode 4770 Mean Rewards 18.44 \n",
            "Episode 4780 Mean Rewards 18.37 \n",
            "Episode 4790 Mean Rewards 18.42 \n",
            "Episode 4800 Mean Rewards 18.26 \n",
            "Total steps so far: 10419256\n",
            "Episode 4810 Mean Rewards 18.16 \n",
            "Episode 4820 Mean Rewards 18.06 \n",
            "Episode 4830 Mean Rewards 18.08 \n",
            "Episode 4840 Mean Rewards 18.11 \n",
            "Episode 4850 Mean Rewards 18.07 \n",
            "Episode 4860 Mean Rewards 17.95 \n",
            "Episode 4870 Mean Rewards 17.92 \n",
            "Episode 4880 Mean Rewards 18.00 \n",
            "Episode 4890 Mean Rewards 17.83 \n",
            "Episode 4900 Mean Rewards 17.80 \n",
            "Total steps so far: 10654835\n",
            "Episode 4910 Mean Rewards 17.91 \n",
            "Episode 4920 Mean Rewards 17.86 \n",
            "Episode 4930 Mean Rewards 17.88 \n",
            "Episode 4940 Mean Rewards 17.96 \n",
            "Episode 4950 Mean Rewards 18.03 \n",
            "Episode 4960 Mean Rewards 18.12 \n",
            "Episode 4970 Mean Rewards 18.28 \n",
            "Episode 4980 Mean Rewards 18.15 \n",
            "Episode 4990 Mean Rewards 18.23 \n",
            "Script run in: 11:14:25.201204 seconds\n"
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "t0 = datetime.now()\n",
        "\n",
        "steps = 0\n",
        "\n",
        "training_rewards = []\n",
        "training_loss = []\n",
        "mean_episode_rewards = []\n",
        "episodes_reward = []\n",
        "episodes_loss = []\n",
        "\n",
        "best_score = -21\n",
        "save_score_target = -19\n",
        "\n",
        "current_episode = len(mean_episode_rewards)\n",
        "# steps = 2044480\n",
        "\n",
        "# training_loss = episodes_loss\n",
        "# episodes_reward = mean_episode_rewards\n",
        "# save_score_target = best_score + 0.8\n",
        "\n",
        "for i in range(max_episodes):\n",
        "    \n",
        "    done = False\n",
        "    s0 = env.reset()\n",
        "    ep_rewards = 0\n",
        "    ep_losses = []\n",
        "    agent.reset_frames()\n",
        "    \n",
        "    while not done:\n",
        "        steps += 1\n",
        "        \n",
        "        # taking action\n",
        "        s0, reward, done = agent.take_action(s0)\n",
        "        ep_rewards += reward\n",
        "        \n",
        "        # learning\n",
        "        if steps % network_update_frequency == 0:\n",
        "            loss = agent.learn(batch_size)\n",
        "            ep_losses.append(loss)\n",
        "        \n",
        "        # # syncing\n",
        "        # if steps % network_sync_frequency == 0:\n",
        "        #     copy_network(Q_network, Q_target)\n",
        "\n",
        "        # update epsion\n",
        "        if steps % network_epsilon_update_frequency == 0 and steps >= 50000:\n",
        "            agent.update_epsilon()\n",
        "        \n",
        "    if done:    \n",
        "        # updates after end of episode\n",
        "        episodes_reward.append(ep_rewards)\n",
        "        episodes_loss.append(np.mean(ep_losses))\n",
        "\n",
        "        if len(episodes_reward) >= window:\n",
        "            mean_rewards = np.mean(\n",
        "                                episodes_reward[-window:])\n",
        "            mean_episode_rewards.append(mean_rewards)\n",
        "\n",
        "        else:\n",
        "            mean_rewards = np.mean(\n",
        "                                episodes_reward)\n",
        "            mean_episode_rewards.append(mean_rewards)\n",
        "\n",
        "\n",
        "        if mean_rewards > best_score:\n",
        "            best_score = mean_rewards\n",
        "            if mean_rewards > save_score_target:\n",
        "                    save_score_target = mean_rewards + 0.8\n",
        "                    save_data(i+current_episode, int(100*mean_rewards))\n",
        "                    print(f'Model Saved. Total Steps: {steps}. New best score: {mean_rewards}')\n",
        "                \n",
        "        if i % 10 == 0:\n",
        "            print(\"Episode {:d} Mean Rewards {:.2f} \".format(\n",
        "                        i+current_episode, mean_rewards))\n",
        "        if i % 100 ==0:\n",
        "            print('Total steps so far:', steps)\n",
        "            if agent.epsilon > agent.epsilon_final:\n",
        "                print('Current epsilon:', agent.epsilon)\n",
        "        \n",
        "        if i % 1000 ==0 and i>0:\n",
        "            print(\"Check point\")\n",
        "            save_data(i+current_episode, int(100*mean_rewards))\n",
        "\n",
        "\n",
        "        if mean_rewards >= rewards_target:\n",
        "            print(f\"Environment Solved after {i} episodes and {steps} steps\")\n",
        "            break\n",
        "        \n",
        "\n",
        "            \n",
        "        \n",
        "        \n",
        "dt = (datetime.now() - t0)\n",
        "print('Script run in:', dt, 'seconds')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUfCIvxRdWJx",
        "outputId": "77b2e5c3-addf-46ee-d281-eb6624f7fbe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "steps: 9394934\n"
          ]
        }
      ],
      "source": [
        "print(\"steps:\", steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Pufl5NgReR1z",
        "outputId": "1980f094-c378-44dc-b784-f4f5ae96b7ff"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(mean_rewards)\n",
        "#plt.legend()\n",
        "plt.title(\"Pong Double Q-Network Mean rewards \")\n",
        "plt.xlabel('frames')#, fontsize=18)\n",
        "plt.ylabel('mean rewards')#, fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2j0gkOFYhQu8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Pong Vanilla DQN- Submit.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}